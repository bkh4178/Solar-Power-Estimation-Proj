{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fa93c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install xgboost lightgbm catboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pd.set_option(\"display.width\", 160)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "# âœ… ìµœì¢… ì‚¬ìš©í•  Feature (ë„¤ ìµœê³  ê¸°ë¡ ë…¸íŠ¸ë¶ ê¸°ì¤€ 20ê°œ)\n",
    "final_feats = [\n",
    "    'humidity', 'wind_gust_spd', 'hour', 'doy', 'wind_spd_b',\n",
    "    'ceiling', 'uv_idx', 'appr_temp', 'uv_cloud_adj', 'dow',\n",
    "    'hour_sin', 'doy_sin', 'is_rain', 'rain', 'hour_cos',\n",
    "    'doy_cos', 'snow', 'coord1', 'coord2', 'haze'\n",
    "]\n",
    "target_col = \"nins\"\n",
    "\n",
    "# ë³´ê°„ íŒŒë¼ë¯¸í„°(5ë¶„ ê°„ê²© ê¸°ì¤€ 12ì¹¸=1ì‹œê°„)\n",
    "MAX_GAP = 12\n",
    "DAY_HOURS = (6, 18)  # ì£¼ê°„ í”„ë¡ì‹œ\n",
    "print(\"âœ… Final feature count:\", len(final_feats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d951bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    EPS = 1e-6\n",
    "    out = df.copy()\n",
    "\n",
    "    # time\n",
    "    if not np.issubdtype(out[\"time\"].dtype, np.datetime64):\n",
    "        out[\"time\"] = pd.to_datetime(out[\"time\"])\n",
    "\n",
    "    # ì‹œê°„ íŒŒìƒ\n",
    "    out[\"hour\"] = out[\"time\"].dt.hour.astype(\"int16\")\n",
    "    out[\"dow\"]  = out[\"time\"].dt.dayofweek.astype(\"int16\")\n",
    "    out[\"doy\"]  = out[\"time\"].dt.dayofyear.astype(\"int16\")\n",
    "    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24).astype(\"float32\")\n",
    "    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24).astype(\"float32\")\n",
    "    out[\"doy_sin\"]  = np.sin(2*np.pi*out[\"doy\"]/365).astype(\"float32\")\n",
    "    out[\"doy_cos\"]  = np.cos(2*np.pi*out[\"doy\"]/365).astype(\"float32\")\n",
    "\n",
    "    # ê°•ìˆ˜ ì—¬ë¶€ (ê·œì¹™ ê¸°ë°˜)\n",
    "    if \"precip_1h\" in out.columns:\n",
    "        out[\"is_rain\"] = (pd.to_numeric(out[\"precip_1h\"], errors=\"coerce\").fillna(0) > 0).astype(\"int8\")\n",
    "    elif \"rain\" in out.columns:\n",
    "        out[\"is_rain\"] = (pd.to_numeric(out[\"rain\"], errors=\"coerce\").fillna(0) > 0).astype(\"int8\")\n",
    "    else:\n",
    "        out[\"is_rain\"] = 0\n",
    "\n",
    "    # haze (ê°€ì‹œê±°ë¦¬ ì—­ìˆ˜)\n",
    "    if \"vis\" in out.columns:\n",
    "        vis = pd.to_numeric(out[\"vis\"], errors=\"coerce\").astype(\"float32\")\n",
    "        out[\"haze\"] = (1.0 / (vis + EPS)).astype(\"float32\")\n",
    "    else:\n",
    "        out[\"haze\"] = 0.0\n",
    "\n",
    "    # pv_id ë©”ëª¨ë¦¬ ê²½ê°\n",
    "    if \"pv_id\" in out.columns and not pd.api.types.is_categorical_dtype(out[\"pv_id\"]):\n",
    "        out[\"pv_id\"] = out[\"pv_id\"].astype(\"category\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae3542",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_weather(df: pd.DataFrame, max_gap: int = MAX_GAP) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # ì£¼ê°„ ë§ˆìŠ¤í¬\n",
    "    day_mask = out[\"hour\"].between(DAY_HOURS[0], DAY_HOURS[1])\n",
    "\n",
    "    # ---- ì´ì‚°/ì´ë²¤íŠ¸í˜•: ë³´ê°„ ê¸ˆì§€, ì§§ì€ ê²°ì†ë§Œ ì•ë’¤ë¡œ ë©”ì›€ ----\n",
    "    if \"is_rain\" in out.columns:\n",
    "        s = out.groupby(\"pv_id\", observed=True)[\"is_rain\"].transform(\n",
    "            lambda x: x.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # ë‚¨ì€ NaNì€ ì¤‘ì•™ê°’ìœ¼ë¡œ ë‹¨ê³„ì  ì±„ì›€\n",
    "        if s.isna().any():\n",
    "            s = s.fillna(out.groupby([\"pv_id\",\"hour\"], observed=True)[\"is_rain\"].transform(\"median\"))\n",
    "        if s.isna().any():\n",
    "            s = s.fillna(out.groupby(\"pv_id\", observed=True)[\"is_rain\"].transform(\"median\"))\n",
    "        if s.isna().any():\n",
    "            s = s.fillna(0)\n",
    "        out[\"is_rain\"] = s.astype(\"float32\")\n",
    "\n",
    "    # ---- ì—°ì†í˜•: ì„ í˜• ë³´ê°„ + ì¤‘ì•™ê°’ ë°±ì—… ----\n",
    "    cont_cols = [c for c in [\n",
    "        \"humidity\", \"wind_spd_b\", \"wind_gust_spd\", \"ceiling\", \"appr_temp\", \"uv_idx\", \"haze\"\n",
    "    ] if c in out.columns]\n",
    "\n",
    "    for c in cont_cols:\n",
    "        # uv_idxëŠ” ì£¼ê°„ë§Œ ë³´ê°„, ì•¼ê°„ì€ 0 ìœ ì§€\n",
    "        if c == \"uv_idx\":\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").astype(\"float32\")\n",
    "            out[c] = out[c].where(day_mask, 0.0)\n",
    "\n",
    "        s_lin = out.groupby(\"pv_id\", observed=True)[c].transform(\n",
    "            lambda x: x.interpolate(method=\"linear\", limit=max_gap, limit_direction=\"both\")\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # ì”ì—¬ ê²°ì¸¡ ë‹¨ê³„ì  ëŒ€ì²´: (pv,hour) â†’ pv â†’ ì „ì²´\n",
    "        if s_lin.isna().any():\n",
    "            s_lin = s_lin.fillna(out.groupby([\"pv_id\",\"hour\"], observed=True)[c].transform(\"median\"))\n",
    "        if s_lin.isna().any():\n",
    "            s_lin = s_lin.fillna(out.groupby(\"pv_id\", observed=True)[c].transform(\"median\"))\n",
    "        if s_lin.isna().any():\n",
    "            s_lin = s_lin.fillna(s_lin.median())\n",
    "\n",
    "        out[c] = s_lin.astype(\"float32\")\n",
    "\n",
    "    # ---- rain/snow: ë³´ê°„í•˜ì§€ ì•Šê³  0ìœ¼ë¡œ ìµœì†Œ ëŒ€ì²´ + ë¬¼ë¦¬ í´ë¦½ ----\n",
    "    for c in [\"rain\", \"snow\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).clip(lower=0).astype(\"float32\")\n",
    "\n",
    "    # ---- ë¬¼ë¦¬ ë²”ìœ„ í´ë¦½ ----\n",
    "    clip_map = {\n",
    "        \"humidity\": (0, 100),\n",
    "        \"uv_idx\":   (0, None),\n",
    "        \"wind_spd_b\": (0, None),\n",
    "        \"wind_gust_spd\": (0, None),\n",
    "        \"ceiling\": (0, None),\n",
    "        \"appr_temp\": (None, None),  # ì˜¨ë„ëŠ” ìŒìˆ˜ ê°€ëŠ¥\n",
    "        \"haze\":    (0, None),\n",
    "        \"rain\":    (0, None),\n",
    "        \"snow\":    (0, None),\n",
    "    }\n",
    "    for c,(lo,hi) in clip_map.items():\n",
    "        if c in out.columns:\n",
    "            x = out[c].astype(\"float32\")\n",
    "            if lo is not None: x = np.maximum(x, lo)\n",
    "            if hi is not None: x = np.minimum(x, hi)\n",
    "            out[c] = x.astype(\"float32\")\n",
    "\n",
    "    # ---- uv_cloud_adj ì¬ê³„ì‚° ----\n",
    "    EPS = 1e-6\n",
    "    if \"uv_idx\" in out.columns:\n",
    "        if \"cloud_a\" in out.columns:\n",
    "            cloud_a_norm = pd.to_numeric(out[\"cloud_a\"], errors=\"coerce\").astype(\"float32\")\n",
    "            cloud_a_norm = cloud_a_norm / (float(cloud_a_norm.max()) + EPS)\n",
    "            out[\"uv_cloud_adj\"] = (out[\"uv_idx\"] * (1 - cloud_a_norm)).astype(\"float32\")\n",
    "        else:\n",
    "            out[\"uv_cloud_adj\"] = out[\"uv_idx\"].astype(\"float32\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f954528",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"ğŸš€ Loading train & test CSVs ...\")\n",
    "train_raw = pd.read_csv(\"train.csv\", low_memory=True, memory_map=True)\n",
    "test_raw  = pd.read_csv(\"test.csv\",  low_memory=True, memory_map=True)\n",
    "print(f\"âœ… Raw shapes â€” Train: {train_raw.shape} | Test: {test_raw.shape}\")\n",
    "\n",
    "train_raw = add_features(train_raw)\n",
    "test_raw  = add_features(test_raw)\n",
    "\n",
    "# íƒ€ê¹ƒ ì •ì œ\n",
    "train_raw[target_col] = pd.to_numeric(train_raw[target_col], errors=\"coerce\").astype(\"float32\")\n",
    "before = len(train_raw)\n",
    "train_raw = train_raw.dropna(subset=[target_col])\n",
    "print(f\"ğŸ”§ Dropped {before - len(train_raw)} rows with NaN target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09219f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "# ëˆ„ìˆ˜ ë°©ì§€: split í›„ ê°ê° ë³´ê°„\n",
    "train_df = interpolate_weather(train_df, max_gap=MAX_GAP)\n",
    "valid_df = interpolate_weather(valid_df, max_gap=MAX_GAP)\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ + NaN ìµœì†Œ ëŒ€ì²´\n",
    "train_df = train_df[[c for c in (final_feats + [target_col]) if c in train_df.columns]].copy()\n",
    "valid_df = valid_df[[c for c in (final_feats + [target_col]) if c in valid_df.columns]].copy()\n",
    "train_df[final_feats] = train_df[final_feats].astype(\"float32\").fillna(0)\n",
    "valid_df[final_feats] = valid_df[final_feats].astype(\"float32\").fillna(0)\n",
    "\n",
    "X_tr, y_tr = train_df[final_feats], train_df[target_col].values\n",
    "X_va, y_va = valid_df[final_feats], valid_df[target_col].values\n",
    "print(\"âœ… Split OK â€”\", f\"train: {X_tr.shape}, valid: {X_va.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472b0ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1) XGBoost (xgb.train)\n",
    "# -----------------------\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=X_tr.columns.tolist())\n",
    "dvalid = xgb.DMatrix(X_va, label=y_va, feature_names=X_va.columns.tolist())\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"max_depth\": 7,\n",
    "    \"min_child_weight\": 4,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 42,\n",
    "    \"tree_method\": \"hist\",  # GPUë©´ \"gpu_hist\"\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ Training XGBoost...\")\n",
    "xgb_model = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=4000,\n",
    "    evals=[(dtrain,\"train\"),(dvalid,\"valid\")],\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=200\n",
    ")\n",
    "xgb_best_n = xgb_model.best_iteration + 1\n",
    "pred_xgb_va = xgb_model.predict(dvalid, iteration_range=(0, xgb_model.best_iteration + 1))\n",
    "mae_xgb = mean_absolute_error(y_va, pred_xgb_va)\n",
    "print(f\"âœ… XGB best_iter={xgb_best_n}, valid MAE={mae_xgb:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) LightGBM\n",
    "# -----------------------\n",
    "print(\"\\nğŸš€ Training LightGBM...\")\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "lgb_valid = lgb.Dataset(X_va, label=y_va, reference=lgb_train)\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"num_leaves\": 64,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    params=lgb_params,\n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=8000,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    valid_names=[\"train\",\"valid\"],\n",
    "    callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
    ")\n",
    "lgb_best_n = lgb_model.best_iteration\n",
    "pred_lgb_va = lgb_model.predict(X_va, num_iteration=lgb_best_n)\n",
    "mae_lgb = mean_absolute_error(y_va, pred_lgb_va)\n",
    "print(f\"âœ… LGB best_iter={lgb_best_n}, valid MAE={mae_lgb:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 3) CatBoost\n",
    "# -----------------------\n",
    "print(\"\\nğŸš€ Training CatBoost...\")\n",
    "cat_model = CatBoostRegressor(\n",
    "    loss_function=\"MAE\",\n",
    "    iterations=10000,\n",
    "    learning_rate=0.04,\n",
    "    depth=8,\n",
    "    random_seed=42,\n",
    "    verbose=200\n",
    ")\n",
    "cat_model.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "cat_best_n = cat_model.get_best_iteration()\n",
    "pred_cat_va = cat_model.predict(X_va)\n",
    "mae_cat = mean_absolute_error(y_va, pred_cat_va)\n",
    "print(f\"âœ… CAT best_iter={cat_best_n}, valid MAE={mae_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fa3f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pred_mean3 = (pred_xgb_va + pred_lgb_va + pred_cat_va) / 3.0\n",
    "mae_mean3 = mean_absolute_error(y_va, pred_mean3)\n",
    "print(f\"âœ… Mean(3 models) valid MAE = {mae_mean3:.4f}\")\n",
    "\n",
    "# Weighted search: wx + wl + wc = 1, each >=0\n",
    "# step ì¤„ì´ë©´ ë” ì •ë°€í•˜ì§€ë§Œ ì˜¤ë˜ ê±¸ë¦¼\n",
    "step = 0.05\n",
    "\n",
    "best = {\"mae\": 1e18, \"wx\": None, \"wl\": None, \"wc\": None}\n",
    "grid = np.arange(0, 1 + 1e-9, step)\n",
    "\n",
    "for wx in grid:\n",
    "    for wl in grid:\n",
    "        wc = 1.0 - wx - wl\n",
    "        if wc < -1e-12:\n",
    "            continue\n",
    "        if wc < 0:\n",
    "            wc = 0.0\n",
    "\n",
    "        pred = wx * pred_xgb_va + wl * pred_lgb_va + wc * pred_cat_va\n",
    "        mae = mean_absolute_error(y_va, pred)\n",
    "        if mae < best[\"mae\"]:\n",
    "            best = {\"mae\": mae, \"wx\": float(wx), \"wl\": float(wl), \"wc\": float(wc)}\n",
    "\n",
    "print(f\"ğŸ Best weighted(3 models): wx={best['wx']:.2f}, wl={best['wl']:.2f}, wc={best['wc']:.2f} | MAE={best['mae']:.4f}\")\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ validì—ì„œ ê°€ì¥ ì¢‹ì€ ì¡°í•© ì„ íƒ\n",
    "# (mean vs weighted ì¤‘ best)\n",
    "use_weighted = best[\"mae\"] <= mae_mean3\n",
    "print(\"âœ… Choose:\", \"WEIGHTED\" if use_weighted else \"MEAN\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
