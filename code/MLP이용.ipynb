{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGB Regressor + Tab_MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuOlFupe4C8J",
        "outputId": "42b22138-5ea8-44bc-b5ec-43a89ec08911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "DEVICE: cuda\n",
            "Final feature count: 20\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import load_npz\n",
        "import numpy as np\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import os, math, gc, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "\n",
        "# ============================================================\n",
        "# Colab: Tab-MLP 학습 → test_itp 예측 → CSV 생성 → Google Drive 저장\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# ========== 0) 구글 드라이브 마운트 & 출력 경로 ==========\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive\"\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "\n",
        "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")  # 파일명에 붙일 타임스탬프\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "\n",
        "# 최종 사용할 Feature (20개)\n",
        "final_feats = [\n",
        "    'humidity', 'wind_gust_spd', 'hour', 'doy', 'wind_spd_b',\n",
        "    'ceiling', 'uv_idx', 'appr_temp', 'uv_cloud_adj', 'dow',\n",
        "    'hour_sin', 'doy_sin', 'is_rain', 'rain', 'hour_cos',\n",
        "    'doy_cos', 'snow', 'coord1', 'coord2', 'haze'\n",
        "]\n",
        "\n",
        "target_col = \"nins\"\n",
        "\n",
        "# 보간 파라미터\n",
        "MAX_GAP = 12  # 연속 결손 허용 길이\n",
        "DAY_HOURS = (6, 18)  # 주간\n",
        "print(\"Final feature count:\", len(final_feats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGB 예측값 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aQvF7QV4CSW"
      },
      "outputs": [],
      "source": [
        "# preprocessing_train.py의 train, valid set 불러오기\n",
        "# train, valid는 시계열 데이터의 특성 상 time leakage를 없애기 위해 관측소별 분리\n",
        "train = pd.read_parquet('X_train_t.parquet')\n",
        "valid = pd.read_parquet('X_valid_t.parquet')\n",
        "\n",
        "# feature/label 분리\n",
        "X_train = train[final_feats]\n",
        "X_valid = valid[final_feats]\n",
        "y_train = train[target_col].values\n",
        "y_valid = valid[target_col].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rRr0ryAuEd5"
      },
      "outputs": [],
      "source": [
        "# 학습 완료한 최종 xgb 모델 불러오기\n",
        "xgb_model_path = \"xgb_full_final.json\"\n",
        "\n",
        "xgb_reg = xgb.XGBRegressor()\n",
        "xgb_reg.load_model(xgb_model_path)\n",
        "\n",
        "xgb_reg.set_params(\n",
        "    tree_method=\"hist\",\n",
        "    device=\"cuda\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "def xgb_predict(df):\n",
        "    kwargs = {}\n",
        "    # best iteration 불러와서 range 설정\n",
        "    if getattr(xgb_reg, \"best_iteration\", None) is not None:\n",
        "        kwargs[\"iteration_range\"] = (0, xgb_reg.best_iteration + 1)\n",
        "    return xgb_reg.predict(df, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz64dZ1pHRG-",
        "outputId": "e786f8a9-96a1-41a3-d487-ac2a2ac956a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [04:07:22] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  return func(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE XGB train : 30.09756088256836\n",
            "MAE XGB valid : 30.45993423461914\n"
          ]
        }
      ],
      "source": [
        "# 1) XGB 예측값 (train, valid 각각)\n",
        "y_pred_tr_xgb = xgb_predict(X_train).astype(np.float32)  # (N_tr,)\n",
        "y_pred_va_xgb = xgb_predict(X_valid).astype(np.float32)  # (N_va,)\n",
        "\n",
        "# 2) residual = y - y_hat_xgb\n",
        "res_tr = (y_train - y_pred_tr_xgb).astype(np.float32).reshape(-1, 1)\n",
        "res_va = (y_valid - y_pred_va_xgb).astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "print(\"MAE XGB train :\", mean_absolute_error(y_train, y_pred_tr_xgb))\n",
        "print(\"MAE XGB valid :\", mean_absolute_error(y_valid, y_pred_va_xgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT8VR9nEurD1"
      },
      "outputs": [],
      "source": [
        "X_train_nn = X_train.copy()\n",
        "X_valid_nn = X_valid.copy()\n",
        "\n",
        "X_train_nn[\"xgb_pred\"] = y_pred_tr_xgb   # 길이 n_sample\n",
        "X_valid_nn[\"xgb_pred\"] = y_pred_va_xgb   # 길이 n_va\n",
        "\n",
        "feature_cols_nn = list(X_train_nn.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "data loader & 결측 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG7vhoUY3aH6"
      },
      "outputs": [],
      "source": [
        "# 1) 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train_np = scaler.fit_transform(X_train_nn.values.astype(np.float32))\n",
        "X_valid_np = scaler.transform(X_valid_nn.values.astype(np.float32))\n",
        "\n",
        "class ArrayDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))  # (N,1)\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "\n",
        "train_ds = ArrayDataset(X_train_np, res_tr)\n",
        "valid_ds = ArrayDataset(X_valid_np, res_va)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=8192, shuffle=True,\n",
        "    num_workers=2, pin_memory=True, drop_last=True\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_ds, batch_size=16384, shuffle=False,\n",
        "    num_workers=1, pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ndZh-_vYUbG",
        "outputId": "59e6df05-f5ca-453b-bdc7-63ab0eb07a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_np   NaN: False Inf: False\n",
            "X_valid_np   NaN: False Inf: False\n",
            "res_tr       NaN: False Inf: False\n",
            "res_va       NaN: False Inf: False\n"
          ]
        }
      ],
      "source": [
        "# 결측 & inf 체크\n",
        "print(\"X_train_np   NaN:\", np.isnan(X_train_np).any(), \"Inf:\", np.isinf(X_train_np).any())\n",
        "print(\"X_valid_np   NaN:\", np.isnan(X_valid_np).any(), \"Inf:\", np.isinf(X_valid_np).any())\n",
        "print(\"res_tr       NaN:\", np.isnan(res_tr).any(),     \"Inf:\", np.isinf(res_tr).any())\n",
        "print(\"res_va       NaN:\", np.isnan(res_va).any(),     \"Inf:\", np.isinf(res_va).any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MLP class definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz0Few387H1c"
      },
      "outputs": [],
      "source": [
        "# Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, d, p=0.15):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d, d)\n",
        "        self.act  = nn.GELU()\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.lin2 = nn.Linear(d, d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = x\n",
        "        x = self.lin1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.lin2(x)\n",
        "        x = x + skip\n",
        "        return x\n",
        "\n",
        "\n",
        "# TabMLP_Attn\n",
        "class TabMLP_Attn(nn.Module):\n",
        "    \"\"\"\n",
        "    Tabular MLP + Multihead Attention 하이브리드\n",
        "    - in_dim: 입력 피처 개수\n",
        "    - hidden: MLP latent 차원\n",
        "    - blocks: ResidualBlock 개수\n",
        "    - p: MLP dropout\n",
        "    - n_heads: MultiheadAttention head 수\n",
        "    - n_tokens: latent를 몇 개 토큰으로 펼칠지\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden=384, blocks=4, p=0.15, n_heads=4, n_tokens=8, in_drop=0.05, attn_drop=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dim   = in_dim\n",
        "        self.hidden   = hidden\n",
        "        self.n_tokens = n_tokens\n",
        "\n",
        "        # 1) 입력 정규화 + input dropout + 첫 Linear\n",
        "        self.in_bn   = nn.BatchNorm1d(in_dim)\n",
        "        self.in_drop = nn.Dropout(in_drop)\n",
        "        self.fc_in   = nn.Linear(in_dim, hidden)\n",
        "\n",
        "        # 2) Residual MLP blocks : ResidualBlock을 blocks만큼 쌓음, modules = [ResidualBock(hidden), ResidualBock(hidden), ResidualBock(hidden)]\n",
        "        # self.blocks = [module[0],module[1],module[2]]\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[ResidualBlock(hidden, p=p) for _ in range(blocks)]\n",
        "        )\n",
        "\n",
        "        # 3) Latent → 토큰 생성\n",
        "        #   (B, hidden) → (B, n_tokens, hidden)\n",
        "        self.token_proj = nn.Linear(hidden, hidden * n_tokens)\n",
        "\n",
        "        # 4) Attention 블록\n",
        "        self.attn_norm = nn.LayerNorm(hidden)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden,\n",
        "            num_heads=n_heads,\n",
        "            dropout=attn_drop,\n",
        "            batch_first=True,  # (B, seq, dim) 형식 사용\n",
        "        )\n",
        "\n",
        "        # 5) Attention 출력 정리 + 최종 head\n",
        "        self.post_attn_norm = nn.LayerNorm(hidden)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, in_dim)\n",
        "        # 1) 입력 정규화 + 드롭아웃 + 첫 FC(Linear trans.)\n",
        "        x = self.in_bn(x)\n",
        "        x = self.in_drop(x)\n",
        "        x = self.fc_in(x)           # (B, hidden)\n",
        "\n",
        "        # 2) Residual MLP\n",
        "        x = self.blocks(x)          # (B, hidden)\n",
        "\n",
        "        # 3) 토큰 생성\n",
        "        # (B, hidden) → (B, n_tokens, hidden)\n",
        "        B = x.size(0)\n",
        "        tokens = self.token_proj(x)      # (B, hidden * n_tokens)\n",
        "        tokens = tokens.view(B, self.n_tokens, self.hidden)  # (B, T, H)\n",
        "\n",
        "        # 4) Attention\n",
        "        tokens = self.attn_norm(tokens)          # LN → 안정화\n",
        "        att_out, _ = self.attn(tokens, tokens, tokens)  # (B, T, H)\n",
        "\n",
        "        # 5) Pooling (토큰 평균) + head\n",
        "        att_out = self.post_attn_norm(att_out)\n",
        "        pooled = att_out.mean(dim=1)             # (B, H)\n",
        "        out = self.head(pooled)                  # (B, 1)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jiPOnzF7P5m"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter 정의\n",
        "hidden = 384\n",
        "blocks = 3\n",
        "p = 0.2\n",
        "n_heads = 4\n",
        "n_tokens = 8\n",
        "in_drop = 0.05\n",
        "attn_drop = 0.1\n",
        "\n",
        "in_dim = X_train_np.shape[1]\n",
        "\n",
        "model = TabMLP_Attn(in_dim=X_train_np.shape[1],\n",
        "    hidden= hidden,     # 256~512 사이에서 튜닝\n",
        "    blocks= blocks,\n",
        "    p= p,\n",
        "    n_heads= n_heads,\n",
        "    n_tokens= n_tokens,\n",
        "    in_drop= in_drop,\n",
        "    attn_drop= attn_drop\n",
        ").to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvIPylGkfW1w"
      },
      "outputs": [],
      "source": [
        "# parameter nan, inf값 있는지 확인\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if torch.isnan(p).any() or torch.isinf(p).any():\n",
        "        print(\"!! NaN/Inf in param:\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model train & valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gdo81DfbW5F",
        "outputId": "5a49b239-6204-4dff-ed38-71d27daf75dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0] train=28.6217, valid=29.1141\n",
            "[1] train=28.3529, valid=29.0209\n",
            "[2] train=28.2144, valid=29.0090\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    weight_decay=3e-4\n",
        ")\n",
        "best_val = float(\"inf\")\n",
        "patience = 2\n",
        "bad = 0\n",
        "\n",
        "# AMP 끄기\n",
        "use_amp = False\n",
        "scaler_amp = torch.amp.GradScaler(device=\"cuda\", enabled=False)\n",
        "\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(DEVICE).float()\n",
        "        yb = yb.to(DEVICE).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(\"cuda\", enabled=False):   # ← AMP 완전 off\n",
        "            pred_res = model(xb)\n",
        "            loss = criterion(pred_res, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(train_ds)\n",
        "\n",
        "    # valid도 AMP 끄고\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=False):\n",
        "        for xb, yb in valid_loader:\n",
        "            xb = xb.to(DEVICE).float()\n",
        "            yb = yb.to(DEVICE).float()\n",
        "            pred_res = model(xb)\n",
        "            loss = criterion(pred_res, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(valid_ds)\n",
        "\n",
        "    print(f\"[{epoch}] train={train_loss:.4f}, valid={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val - 1e-4:\n",
        "        best_val = val_loss\n",
        "        bad = 0\n",
        "        best_state = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"scaler_mean\": scaler.mean_,\n",
        "            \"scaler_scale\": scaler.scale_,\n",
        "            \"feature_cols_nn\": feature_cols_nn,\n",
        "        }\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQYgeVkb3aMp"
      },
      "outputs": [],
      "source": [
        "# valid용 NN 입력 만들기\n",
        "X_valid_nn_pred = X_valid.copy()\n",
        "X_valid_nn_pred[\"xgb_pred\"] = y_pred_va_xgb\n",
        "X_valid_nn_pred = X_valid_nn_pred.reindex(columns=best_state[\"feature_cols_nn\"], fill_value=0)\n",
        "\n",
        "# 스케일링 복구\n",
        "scaler_inf = StandardScaler()\n",
        "scaler_inf.mean_ = best_state[\"scaler_mean\"]\n",
        "scaler_inf.scale_ = best_state[\"scaler_scale\"]\n",
        "scaler_inf.var_ = scaler_inf.scale_**2\n",
        "scaler_inf.n_features_in_ = len(best_state[\"feature_cols_nn\"])\n",
        "\n",
        "X_valid_np_pred = scaler_inf.transform(X_valid_nn_pred.values.astype(np.float32))\n",
        "\n",
        "def predict_residual_np(X_np, batch=32768):\n",
        "    preds = []\n",
        "    n = len(X_np)\n",
        "    for i in range(0, n, batch):\n",
        "        xb = torch.from_numpy(X_np[i:i+batch]).to(DEVICE)\n",
        "        with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=(DEVICE==\"cuda\")):\n",
        "            out = model(xb).squeeze().float().cpu().numpy()\n",
        "        preds.append(out)\n",
        "        del xb, out\n",
        "        torch.cuda.empty_cache()\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "res_va_pred = predict_residual_np(X_valid_np_pred)  # (N_va,)\n",
        "\n",
        "\n",
        "# XGB 단독\n",
        "mae_xgb = mean_absolute_error(y_valid, y_pred_va_xgb)\n",
        "\n",
        "# 스태킹 (XGB + NN residual)\n",
        "y_valid_final = y_pred_va_xgb + res_va_pred\n",
        "y_valid_final = np.clip(y_valid_final, 0, 1200)\n",
        "mae_stack = mean_absolute_error(y_valid, y_valid_final)\n",
        "\n",
        "print(\"MAE XGB      :\", mae_xgb)\n",
        "print(\"MAE Stacking :\", mae_stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test set 예측 후 csv생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCX4TuvDwWzw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== 1) 데이터 불러오기 ==========\n",
        "# test_itp: 보간 완료된 테스트 데이터셋 (CSV/Parquet 어떤 형식이든 OK)\n",
        "TEST_ITP_PATH = \"test_itp.parquet\"\n",
        "df_test = pd.read_parquet(TEST_ITP_PATH)\n",
        "\n",
        "# 메타 컬럼(제출 양식용) 확정\n",
        "meta_cols = [c for c in [\"time\", \"pv_id\", \"type\"] if c in df_test.columns]\n",
        "sub_meta = df_test[meta_cols].copy() if meta_cols else pd.DataFrame()\n",
        "\n",
        "# 모델 입력 피처 컬럼: train 기준으로 test 정렬(학습 당시의 열 순서와 동일해야 함)\n",
        "feature_cols = final_feats\n",
        "\n",
        "# test_itp를 학습 컬럼 순서에 맞추고, 없는 열은 0으로 채움\n",
        "X_test = df_test.reindex(columns=feature_cols, fill_value=0)\n",
        "# 숫자형 캐스팅(혹시 object가 섞였을 경우 대비)\n",
        "X_test = X_test.apply(pd.to_numeric, errors=\"coerce\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxxFYYuN4iJV"
      },
      "outputs": [],
      "source": [
        "# 1) XGB로 test 예측\n",
        "y_pred_te_xgb = xgb_predict(X_test).astype(np.float32)\n",
        "\n",
        "# 2) NN 입력 구성 (X_test + xgb_pred)\n",
        "X_test_nn = X_test.copy()\n",
        "X_test_nn[\"xgb_pred\"] = y_pred_te_xgb\n",
        "X_test_nn = X_test_nn.reindex(columns=best_state[\"feature_cols_nn\"], fill_value=0)\n",
        "\n",
        "X_test_np = scaler_inf.transform(X_test_nn.values.astype(np.float32))\n",
        "\n",
        "# 3) residual 예측\n",
        "res_te_pred = predict_residual_np(X_test_np)\n",
        "\n",
        "# 4) 최종 예측 = XGB + residual\n",
        "y_pred_final = y_pred_te_xgb + res_te_pred\n",
        "y_pred_final = np.clip(y_pred_final, 0, 1200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKsV-i764iLN"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# 2) 최종 예측 반영\n",
        "# ------------------------------------------\n",
        "submission = sub_meta.copy()\n",
        "submission['nins'] = y_pred_final.astype(np.float32)\n",
        "\n",
        "# 혹시라도 순서 꼬이면 정렬\n",
        "submission = submission[['time', 'pv_id', 'type', 'nins']]\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3) CSV 저장 (현재 작업 디렉토리)\n",
        "# ------------------------------------------\n",
        "save_path = \"submission_XGBMLP.csv\"\n",
        "submission.to_csv(save_path, index=False, encoding='utf-8')\n",
        "print(\"파일 저장됨:\", save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw-eYuXFpKHT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/OIBC/submission_XGBMLP.csv\"\n",
        "\n",
        "shutil.copy(\"submission_XGBMLP.csv\", drive_path)\n",
        "print(\"Google Drive로 저장 완료:\", drive_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myproj312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
