{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980ed366-eeda-4b13-8d25-68a0200e1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup done.\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 0: Setup =====\n",
    "import os, gc, warnings, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "# 경로는 니 노트북 기준으로 맞춰줘\n",
    "DATA_DIR = \"./\"   # train.csv, test.csv, sample_submission.csv 있는 폴더\n",
    "SAVE_SUB = \"./result_submission.csv\"\n",
    "\n",
    "TIME_COL = \"time\"\n",
    "PV_COL   = \"pv_id\"\n",
    "TARGET   = \"nins\"        # 일사량 타깃\n",
    "\n",
    "# 메모리 다운캐스트\n",
    "def downcast_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    for c in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "# 안전 merge (좌측키 보존, 중복열 접미사 방지)\n",
    "def safe_merge(left: pd.DataFrame, right: pd.DataFrame, on: str, how=\"left\") -> pd.DataFrame:\n",
    "    right = right.loc[:, ~right.columns.duplicated()]\n",
    "    return left.merge(right, on=on, how=how, copy=False)\n",
    "\n",
    "print(\"Setup done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a7c091-69bc-4d20-b91d-e4d5088b145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19236948, 12) (2838240, 11) loaded.\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: Load CSVs (thin) =====\n",
    "# 필요한 열만 지정 (파일 열 목록에 맞춰 적당히 수정)\n",
    "base_cols = [TIME_COL, PV_COL, TARGET]\n",
    "# 대표 변수군(예시) — 파일 실제 열 이름에 맞춰 조정해\n",
    "REP_VARS = [\n",
    "    \"temp_a\",\"humidity\",\"cloud\",\"rain\",\"wind_spd_a\",\"wind_spd_b\",\n",
    "    \"wind_gust_spd\",\"ground_press\"\n",
    "]\n",
    "# 풍향 관련(사인/코사인 변환해 쓸 수 있음)\n",
    "WIND_DIR_COLS = [\"wind_dir_a\",\"wind_dir_b\"]\n",
    "\n",
    "AUX_KEEP_RAW = []  # 유지하고 싶은 기타 원본 열 있으면 넣기\n",
    "\n",
    "keep_cols = list(dict.fromkeys(base_cols + REP_VARS + WIND_DIR_COLS + AUX_KEEP_RAW))\n",
    "\n",
    "dtype_hint = {c: \"float32\" for c in keep_cols if c not in [TIME_COL, PV_COL, TARGET]}\n",
    "dtype_hint[TARGET] = \"float32\"\n",
    "\n",
    "def read_csv_thin(path: str, usecols: List[str]) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        usecols=lambda c: c in usecols,\n",
    "        parse_dates=[TIME_COL],\n",
    "        dtype=dtype_hint,\n",
    "        low_memory=True,\n",
    "        memory_map=True\n",
    "    )\n",
    "\n",
    "train = read_csv_thin(os.path.join(DATA_DIR, \"train.csv\"), keep_cols)\n",
    "test  = read_csv_thin(os.path.join(DATA_DIR, \"test.csv\"),  [c for c in keep_cols if c != TARGET])\n",
    "\n",
    "# pv_id를 카테고리로 (메모리↓)\n",
    "if train[PV_COL].dtype != \"category\":\n",
    "    train[PV_COL] = train[PV_COL].astype(\"category\")\n",
    "if test[PV_COL].dtype != \"category\":\n",
    "    test[PV_COL] = test[PV_COL].astype(\"category\")\n",
    "\n",
    "train = downcast_df(train); test = downcast_df(test)\n",
    "print(train.shape, test.shape, \"loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64701200-edb1-41c1-8565-92451820c3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords attached: True\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: Attach coords by pv_id =====\n",
    "# coord_map: [pv_id, coord1(lat), coord2(lon)] 형태의 DataFrame이 있다면 여기로 읽어 붙이기.\n",
    "# 없다면 전국 평균(대략)으로 기본값 부여. (나중에 실제 좌표가 있으면 merge만 바꿔주면 됨)\n",
    "\n",
    "def attach_coords_by_pvid(df: pd.DataFrame, coord_map: pd.DataFrame=None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if coord_map is None or not {\"pv_id\",\"coord1\",\"coord2\"}.issubset(set(coord_map.columns)):\n",
    "        # 기본값 (대한민국 중심 근사)\n",
    "        default_lat, default_lon = 36.5, 127.8\n",
    "        # pv_id별 개별 좌표가 없다면, 고유 pv에 동일 기본값 부여\n",
    "        uniq = pd.DataFrame({PV_COL: out[PV_COL].cat.categories})\n",
    "        uniq[\"coord1\"] = default_lat\n",
    "        uniq[\"coord2\"] = default_lon\n",
    "        coord_map = uniq\n",
    "\n",
    "    # 안전 머지\n",
    "    out = safe_merge(out, coord_map.rename(columns={PV_COL:\"pv_id\"}), on=\"pv_id\", how=\"left\")\n",
    "    # 수치화 + 기본값 보정\n",
    "    out[\"coord1\"] = pd.to_numeric(out[\"coord1\"], errors=\"coerce\").fillna(36.5).astype(\"float32\")\n",
    "    out[\"coord2\"] = pd.to_numeric(out[\"coord2\"], errors=\"coerce\").fillna(127.8).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# 만약 coord_map.csv가 따로 있으면 아래 주석 해제해서 사용\n",
    "# coord_map = pd.read_csv(os.path.join(DATA_DIR, \"coord_map.csv\"))\n",
    "coord_map = None\n",
    "\n",
    "train = attach_coords_by_pvid(train, coord_map)\n",
    "test  = attach_coords_by_pvid(test,  coord_map)\n",
    "print(\"coords attached:\", set([\"coord1\",\"coord2\"]).issubset(train.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa9035d-7ed6-4012-997c-3df1ce998d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pv_id dtype: category | nunique: 183\n",
      "test  pv_id dtype: category | nunique: 27\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2.9: Force categorical for pv_id =====\n",
    "import pandas as pd\n",
    "\n",
    "def ensure_categorical_pvid(df: pd.DataFrame, col: str = PV_COL):\n",
    "    # 문자열로 강제 후 카테고리화 (혼재형/숫자형도 안전)\n",
    "    if df[col].dtype != \"category\":\n",
    "        df[col] = df[col].astype(str).astype(\"category\")\n",
    "    return df\n",
    "\n",
    "train = ensure_categorical_pvid(train, PV_COL)\n",
    "test  = ensure_categorical_pvid(test,  PV_COL)\n",
    "\n",
    "print(\"train pv_id dtype:\", train[PV_COL].dtype, \"| nunique:\", train[PV_COL].nunique())\n",
    "print(\"test  pv_id dtype:\", test[PV_COL].dtype,  \"| nunique:\", test[PV_COL].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede4c872-2ae1-439e-bcd9-b40e5491175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split pv counts: 137 46 27\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: Split by pv_id =====\n",
    "# pv_id별 분할: 발전소 편향을 검증에 남겨 일반화 확인\n",
    "pvs = train[PV_COL].cat.categories.tolist()\n",
    "pvs_tr, pvs_va = train_test_split(pvs, test_size=0.25, random_state=SEED, shuffle=True)\n",
    "\n",
    "m_tr = train[PV_COL].isin(pvs_tr)\n",
    "m_va = train[PV_COL].isin(pvs_va)\n",
    "\n",
    "df_tr = train.loc[m_tr].copy()\n",
    "df_va = train.loc[m_va].copy()\n",
    "\n",
    "# 정렬 (롤링 전 필수)\n",
    "df_tr = df_tr.sort_values([PV_COL, TIME_COL])\n",
    "df_va = df_va.sort_values([PV_COL, TIME_COL])\n",
    "test  = test.sort_values([PV_COL, TIME_COL])\n",
    "\n",
    "print(\"Split pv counts:\", df_tr[PV_COL].nunique(), df_va[PV_COL].nunique(), test[PV_COL].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9811e0e-6781-48b0-bdb6-032960a0fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After bfill + dropna: (14401396, 14) (4835519, 14) (2838240, 13)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: Interpolation (pv_id-wise) =====\n",
    "# 규칙: pv_id별로 기상변수 결측치를 bfill. (train은 이후 결측행 drop)\n",
    "fill_cols = [c for c in REP_VARS + WIND_DIR_COLS if c in df_tr.columns]\n",
    "\n",
    "def bfill_by_group(df: pd.DataFrame, cols: List[str], group_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[cols] = out.groupby(group_col, observed=True)[cols].bfill()\n",
    "    return out\n",
    "\n",
    "df_tr = bfill_by_group(df_tr, fill_cols, PV_COL)\n",
    "df_va = bfill_by_group(df_va, fill_cols, PV_COL)\n",
    "test  = bfill_by_group(test,  fill_cols, PV_COL)\n",
    "\n",
    "# train에서 해당 feature 결측행 제거(룰)\n",
    "df_tr = df_tr.dropna(subset=fill_cols)\n",
    "# valid/test는 남겨두고 이후 모델이 다룸 (또는 2차 보정)\n",
    "print(\"After bfill + dropna:\", df_tr.shape, df_va.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c461e7a2-5d89-4d0b-86ea-a011431a349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solar geometry added (hotfix): elev_sin, AM, G0h\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5 (HOTFIX): Solar geometry (precise) =====\n",
    "def ensure_kst_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    time Series -> Asia/Seoul 기준의 tz-naive Series로 변환.\n",
    "    tz가 붙어 있으면 KST로 변환 후 tz 제거, 없으면 그대로 KST 가정.\n",
    "    \"\"\"\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    # tz-aware면 KST로 변환 후 tz 제거\n",
    "    try:\n",
    "        if pd.api.types.is_datetime64tz_dtype(s.dtype):\n",
    "            s = s.dt.tz_convert(\"Asia/Seoul\").dt.tz_localize(None)\n",
    "        else:\n",
    "            # tz-naive면 KST로 가정 (그대로 사용)\n",
    "            pass\n",
    "    except Exception:\n",
    "        # pandas 버전에 따라 is_datetime64tz_dtype 경고/오류 대비\n",
    "        # tz 속성 존재 여부로 분기\n",
    "        if getattr(s.dt, \"tz\", None) is not None:\n",
    "            s = s.dt.tz_convert(\"Asia/Seoul\").dt.tz_localize(None)\n",
    "    return s\n",
    "\n",
    "def solar_geom_inplace(df: pd.DataFrame, lat_col=\"coord1\", lon_col=\"coord2\", time_col=TIME_COL):\n",
    "    # 1) 시간 파트 (Series + .dt 접근자 사용)\n",
    "    t = ensure_kst_series(df[time_col])\n",
    "    doy  = t.dt.dayofyear.astype(\"int16\")\n",
    "    hour = t.dt.hour + t.dt.minute/60.0 + t.dt.second/3600.0  # decimal hour (float)\n",
    "\n",
    "    # 2) 위경도 (rad/deg)\n",
    "    lat = np.deg2rad(df[lat_col].astype(\"float64\").values)\n",
    "    lon = df[lon_col].astype(\"float64\").values\n",
    "\n",
    "    # 3) 태양 적위/시간방정식 (NOAA 근사식)\n",
    "    gamma = 2.0 * np.pi * (doy - 1) / 365.0\n",
    "    decl = (0.006918 \n",
    "            - 0.399912*np.cos(gamma) + 0.070257*np.sin(gamma)\n",
    "            - 0.006758*np.cos(2*gamma) + 0.000907*np.sin(2*gamma)\n",
    "            - 0.002697*np.cos(3*gamma) + 0.00148*np.sin(3*gamma))\n",
    "    eqtime = (229.18*(0.000075 + 0.001868*np.cos(gamma) - 0.032077*np.sin(gamma)\n",
    "                      - 0.014615*np.cos(2*gamma) - 0.040849*np.sin(2*gamma)))  # minutes\n",
    "\n",
    "    # 4) 지역 태양시(Local Solar Time) → 시각각(HRA)\n",
    "    #    한국 표준시는 135E 기준 (UTC+9). 분 단위 보정:\n",
    "    time_offset = eqtime + 4.0*(lon - 135.0)     # minutes\n",
    "    tst = hour*60.0 + time_offset                # true solar time [minutes]\n",
    "    hra = np.deg2rad((tst/4.0) - 180.0)          # hour angle [rad]\n",
    "\n",
    "    # 5) 고도각(sin) 및 클리핑\n",
    "    sin_elev = (np.sin(lat)*np.sin(decl) + np.cos(lat)*np.cos(decl)*np.cos(hra))\n",
    "    sin_elev = np.clip(sin_elev, -1.0, 1.0)\n",
    "    elev_sin = np.maximum(sin_elev, 0.0).astype(\"float32\")  # 지평선 아래 0\n",
    "\n",
    "    # 6) 상대 대기질량 AM (Kasten & Young 1989)\n",
    "    elev = np.rad2deg(np.arcsin(np.clip(sin_elev, 0, 1)))   # deg, 음수구간 0으로 클리핑\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        am = 1.0 / (np.sin(np.deg2rad(np.maximum(elev, 0.0))) + 0.50572 * (6.07995 + elev) ** -1.6364)\n",
    "    am = np.where(elev <= 0, np.nan, am).astype(\"float32\")  # 밤에는 NaN\n",
    "\n",
    "    # 7) 청천 상층대기 수평 일사 G0h\n",
    "    I_sc = 1367.0  # W/m^2\n",
    "    E0 = 1.00011 + 0.034221*np.cos(gamma) + 0.00128*np.sin(gamma) \\\n",
    "                 + 0.000719*np.cos(2*gamma) + 0.000077*np.sin(2*gamma)\n",
    "    G0h = (I_sc * E0 * np.maximum(sin_elev, 0.0)).astype(\"float32\")\n",
    "\n",
    "    df[\"elev_sin\"] = elev_sin\n",
    "    df[\"AM\"]       = am\n",
    "    df[\"G0h\"]      = G0h\n",
    "\n",
    "for _df in (df_tr, df_va, test):\n",
    "    solar_geom_inplace(_df)\n",
    "\n",
    "print(\"Solar geometry added (hotfix): elev_sin, AM, G0h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c54434c-c765-4485-ac99-62b5e201007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pv_id_code added. unique codes: 137 46 27\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 (HOTFIX): Fixed effects (pv_id_code robust) =====\n",
    "def build_pv_codebook(dfs, col=PV_COL):\n",
    "    # 문자열 통일 후 전체 유니온으로 코드북 생성\n",
    "    ser = pd.concat([d[col].astype(str) for d in dfs], ignore_index=True)\n",
    "    uniq = pd.Index(ser.unique())\n",
    "    return {p: i for i, p in enumerate(uniq)}\n",
    "\n",
    "pv2code = build_pv_codebook([df_tr, df_va, test], col=PV_COL)\n",
    "\n",
    "def add_pv_code(df: pd.DataFrame, col=PV_COL):\n",
    "    # 매핑 누락(-1) 방지: fillna(-1) 후 정수 캐스팅\n",
    "    code = df[col].astype(str).map(pv2code)\n",
    "    df[\"pv_id_code\"] = code.fillna(-1).astype(\"int16\")\n",
    "\n",
    "for _df in (df_tr, df_va, test):\n",
    "    add_pv_code(_df)\n",
    "\n",
    "print(\"pv_id_code added. unique codes:\",\n",
    "      df_tr[\"pv_id_code\"].nunique(),\n",
    "      df_va[\"pv_id_code\"].nunique(),\n",
    "      test[\"pv_id_code\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d37c4d-e4cd-449d-a435-3f89f4f1eb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling features added.\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: Light rolling features =====\n",
    "# 3스텝 롤링: precip_3h_sum, cloud_mean_3 (데이터가 5분 간격이면 15분 윈도우)\n",
    "def add_rolling_inplace(df: pd.DataFrame) -> None:\n",
    "    if \"rain\" in df.columns:\n",
    "        df[\"precip_3h_sum\"] = (\n",
    "            df.groupby(PV_COL, observed=True)[\"rain\"]\n",
    "              .rolling(window=3, min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "              .astype(\"float32\")\n",
    "        )\n",
    "    if \"cloud\" in df.columns:\n",
    "        df[\"cloud_mean_3\"] = (\n",
    "            df.groupby(PV_COL, observed=True)[\"cloud\"]\n",
    "              .rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "              .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "for _df in (df_tr, df_va, test):\n",
    "    add_rolling_inplace(_df)\n",
    "\n",
    "print(\"Rolling features added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "514e0152-31cf-400c-bc7e-fb82d65cc00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time & wind encodings added. (TZ-safe)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8 (HOTFIX): Lightweight time & wind encodings (TZ-safe) =====\n",
    "def add_time_covariates(df: pd.DataFrame, time_col=TIME_COL):\n",
    "    # Cell 5에 정의한 ensure_kst_series() 재사용: tz-aware → Asia/Seoul → tz 제거\n",
    "    t = ensure_kst_series(df[time_col])\n",
    "    df[\"hour\"] = t.dt.hour.astype(\"int8\")\n",
    "    df[\"doy\"]  = t.dt.dayofyear.astype(\"int16\")\n",
    "    # 주기 인코딩\n",
    "    df[\"hr_sin1\"] = np.sin(2*np.pi*df[\"hour\"]/24).astype(\"float32\")\n",
    "    df[\"hr_cos1\"] = np.cos(2*np.pi*df[\"hour\"]/24).astype(\"float32\")\n",
    "    df[\"dy_sin1\"] = np.sin(2*np.pi*df[\"doy\"]/365).astype(\"float32\")\n",
    "    df[\"dy_cos1\"] = np.cos(2*np.pi*df[\"doy\"]/365).astype(\"float32\")\n",
    "\n",
    "def add_wind_dir_xy(df: pd.DataFrame):\n",
    "    for c in [\"wind_dir_a\",\"wind_dir_b\"]:\n",
    "        if c in df.columns:\n",
    "            r = np.deg2rad(pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\"))\n",
    "            df[f\"{c}_sin\"] = np.sin(r).astype(\"float32\")\n",
    "            df[f\"{c}_cos\"] = np.cos(r).astype(\"float32\")\n",
    "\n",
    "for _df in (df_tr, df_va, test):\n",
    "    add_time_covariates(_df)\n",
    "    add_wind_dir_xy(_df)\n",
    "\n",
    "print(\"Time & wind encodings added. (TZ-safe)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1496b0e7-aa73-43aa-b85e-87881122df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (7217434, 26) (7217434,) (4835519, 26) (4835519,)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 9: Build train/valid matrices (daytime mask) =====\n",
    "# 주간만(물리적 신호 있는 구간)으로 학습\n",
    "mask_day_tr = (df_tr[\"elev_sin\"] > 0)\n",
    "\n",
    "feature_cols = [c for c in df_tr.columns\n",
    "                if c not in [TARGET, TIME_COL, PV_COL] and not c.startswith(\"Unnamed\")]\n",
    "\n",
    "X_tr = df_tr.loc[mask_day_tr, feature_cols]\n",
    "y_tr = df_tr.loc[mask_day_tr, TARGET].astype(\"float32\")\n",
    "\n",
    "X_va = df_va.loc[:, feature_cols]\n",
    "y_va = df_va[TARGET].astype(\"float32\")\n",
    "\n",
    "# 결측 간단 대체(안정성): 수평 일사/고도/구름 등으로 특정 결측이 생길 수 있음\n",
    "X_tr = X_tr.fillna(0); X_va = X_va.fillna(0)\n",
    "\n",
    "print(\"Shapes:\", X_tr.shape, y_tr.shape, X_va.shape, y_va.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17fb23c9-2740-4150-9a8c-830e783a8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's l1: 82.5333\n",
      "[100]\tvalid_0's l1: 67.2547\n",
      "[150]\tvalid_0's l1: 65.3597\n",
      "[200]\tvalid_0's l1: 64.5577\n",
      "[250]\tvalid_0's l1: 64.266\n",
      "[300]\tvalid_0's l1: 63.9363\n",
      "[350]\tvalid_0's l1: 64.0036\n",
      "[400]\tvalid_0's l1: 63.9183\n",
      "[450]\tvalid_0's l1: 63.667\n",
      "[500]\tvalid_0's l1: 63.492\n",
      "[550]\tvalid_0's l1: 63.5613\n",
      "[600]\tvalid_0's l1: 63.1071\n",
      "[650]\tvalid_0's l1: 63.3618\n",
      "[700]\tvalid_0's l1: 63.216\n",
      "[750]\tvalid_0's l1: 63.1841\n",
      "Early stopping, best iteration is:\n",
      "[596]\tvalid_0's l1: 63.0027\n",
      "Baseline valid MAE: 62.9832 | best_iter=596\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 10: First training (HOTFIX) =====\n",
    "params = dict(\n",
    "    objective=\"mae\",\n",
    "    metric=\"mae\",          # metric 명시\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=256,\n",
    "    min_data_in_leaf=80,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1          # 콘솔 로그 억제\n",
    ")\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=200),\n",
    "    lgb.log_evaluation(period=50)  # 50스텝마다 로그; 조용히 하려면 주석\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_va, y_va)],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# best_iteration_ 가 없을 수 있어 대비\n",
    "best_iter = getattr(model, \"best_iteration_\", None)\n",
    "pred_va = model.predict(X_va, num_iteration=best_iter)\n",
    "pred_va = np.maximum(pred_va, 0.0)\n",
    "mae0 = mean_absolute_error(y_va, pred_va)\n",
    "print(f\"Baseline valid MAE: {mae0:.4f} | best_iter={best_iter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f435c9d8-3c44-4463-9a75-63a60b76397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will drop features (#): 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['coord1', 'coord2', 'pv_id_code'], '')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 11: Permutation importance on valid =====\n",
    "def permutation_importance_mae(\n",
    "    model, X: pd.DataFrame, y: pd.Series, base_mae: float,\n",
    "    n_repeats: int = 1, random_state: int = SEED, sample_frac: float = 0.6\n",
    ") -> pd.DataFrame:\n",
    "    rng = check_random_state(random_state)\n",
    "    # 속도 위해 서브샘플\n",
    "    if sample_frac < 1.0:\n",
    "        idx = rng.choice(X.index.values, size=int(len(X)*sample_frac), replace=False)\n",
    "        Xs, ys = X.loc[idx], y.loc[idx]\n",
    "    else:\n",
    "        Xs, ys = X, y\n",
    "\n",
    "    base_pred = model.predict(Xs, num_iteration=model.best_iteration_)\n",
    "    base_pred = np.maximum(base_pred, 0.0)\n",
    "    base = mean_absolute_error(ys, base_pred)\n",
    "\n",
    "    scores = []\n",
    "    for col in Xs.columns:\n",
    "        worst = []\n",
    "        for _ in range(n_repeats):\n",
    "            Xp = Xs.copy()\n",
    "            Xp[col] = rng.permutation(Xp[col].values)  # 셔플\n",
    "            pred = model.predict(Xp, num_iteration=model.best_iteration_)\n",
    "            pred = np.maximum(pred, 0.0)\n",
    "            m = mean_absolute_error(ys, pred)\n",
    "            worst.append(m)\n",
    "        inc = np.mean(worst) - base\n",
    "        scores.append((col, inc, 100.0 * (inc / (base + 1e-8))))\n",
    "    imp = pd.DataFrame(scores, columns=[\"feature\",\"delta_mae\",\"delta_mae_pct\"]).sort_values(\"delta_mae_pct\", ascending=False)\n",
    "    return imp\n",
    "\n",
    "imp = permutation_importance_mae(model, X_va, y_va, mae0, n_repeats=1, sample_frac=0.6)\n",
    "# 컷 규칙\n",
    "weak_thresh = 0.2  # ΔMAE% ≤ 0.2% 면 약한 피처로 간주\n",
    "weak_feats = imp.loc[imp[\"delta_mae_pct\"] <= weak_thresh, \"feature\"].tolist()\n",
    "\n",
    "# 중복쌍 룰 (데이터에 존재하는 열만 체크)\n",
    "redundant_pairs = [\n",
    "    (\"humidity\", \"rel_hum\"),  # 예시: 실제 파일에 rel_hum이 없으면 무시됨\n",
    "    (\"rain\", \"precip_1h\"),\n",
    "    (\"cloud\", \"cloud_mean_3\"),\n",
    "]\n",
    "to_drop_dup = []\n",
    "for a,b in redundant_pairs:\n",
    "    if a in X_va.columns and b in X_va.columns:\n",
    "        # 더 약한 쪽을 드랍\n",
    "        da = imp.loc[imp[\"feature\"]==a, \"delta_mae_pct\"].values\n",
    "        db = imp.loc[imp[\"feature\"]==b, \"delta_mae_pct\"].values\n",
    "        if len(da)==0 or len(db)==0: \n",
    "            continue\n",
    "        if da[0] >= db[0]:\n",
    "            to_drop_dup.append(b)\n",
    "        else:\n",
    "            to_drop_dup.append(a)\n",
    "\n",
    "drop_feats = sorted(set(weak_feats + to_drop_dup))\n",
    "print(\"Will drop features (#):\", len(drop_feats))\n",
    "drop_feats[:20], (\"...more\" if len(drop_feats)>20 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a97af717-618e-4f4f-8234-cedd5082ec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's l1: 82.3946\n",
      "[100]\tvalid_0's l1: 64.91\n",
      "[150]\tvalid_0's l1: 62.4495\n",
      "[200]\tvalid_0's l1: 61.5328\n",
      "[250]\tvalid_0's l1: 60.898\n",
      "[300]\tvalid_0's l1: 60.4462\n",
      "[350]\tvalid_0's l1: 59.9578\n",
      "[400]\tvalid_0's l1: 59.7419\n",
      "[450]\tvalid_0's l1: 59.4027\n",
      "[500]\tvalid_0's l1: 59.07\n",
      "[550]\tvalid_0's l1: 58.6156\n",
      "[600]\tvalid_0's l1: 58.1861\n",
      "[650]\tvalid_0's l1: 58.2365\n",
      "[700]\tvalid_0's l1: 58.4096\n",
      "[750]\tvalid_0's l1: 58.3632\n",
      "[800]\tvalid_0's l1: 58.4112\n",
      "[850]\tvalid_0's l1: 58.275\n",
      "[900]\tvalid_0's l1: 59.15\n",
      "[950]\tvalid_0's l1: 58.9617\n",
      "[1000]\tvalid_0's l1: 58.808\n",
      "Early stopping, best iteration is:\n",
      "[833]\tvalid_0's l1: 58.0588\n",
      "Retrain valid MAE: 58.0416 | best_iter=833 | Δ=-4.9415\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 12: Retrain with selected features (HOTFIX) =====\n",
    "final_feats = [c for c in feature_cols if c not in drop_feats]\n",
    "\n",
    "X_tr2 = df_tr.loc[mask_day_tr, final_feats].fillna(0)\n",
    "y_tr2 = y_tr.values.astype(\"float32\")\n",
    "X_va2 = df_va.loc[:, final_feats].fillna(0)\n",
    "y_va2 = y_va.values.astype(\"float32\")\n",
    "\n",
    "model2 = lgb.LGBMRegressor(**params)\n",
    "\n",
    "callbacks2 = [\n",
    "    lgb.early_stopping(stopping_rounds=200),\n",
    "    lgb.log_evaluation(period=50)\n",
    "]\n",
    "\n",
    "model2.fit(\n",
    "    X_tr2, y_tr2,\n",
    "    eval_set=[(X_va2, y_va2)],\n",
    "    callbacks=callbacks2\n",
    ")\n",
    "\n",
    "best_iter2 = getattr(model2, \"best_iteration_\", None)\n",
    "pred_va2 = np.maximum(model2.predict(X_va2, num_iteration=best_iter2), 0.0)\n",
    "mae2 = mean_absolute_error(y_va2, pred_va2)\n",
    "print(f\"Retrain valid MAE: {mae2:.4f} | best_iter={best_iter2} | Δ={mae2 - mae0:+.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d004867-8deb-4d03-b30d-38842f28a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./result_submission.csv (2838240, 4) | null nins: 0\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 13 (HOTFIX): Build test features & predict with TZ-safe merge =====\n",
    "X_te = test.loc[:, final_feats].fillna(0)\n",
    "best_iter2 = getattr(model2, \"best_iteration_\", None)\n",
    "pred_te = np.maximum(model2.predict(X_te, num_iteration=best_iter2), 0.0).astype(\"float32\")\n",
    "\n",
    "sub = pd.read_csv(os.path.join(DATA_DIR, \"submission_sample.csv\"))\n",
    "\n",
    "# 머지 키 후보\n",
    "merge_keys = [c for c in [\"time\",\"pv_id\",\"type\"] if c in sub.columns and c in test.columns]\n",
    "\n",
    "def normalize_time_key_inplace(df: pd.DataFrame, col: str = \"time\"):\n",
    "    if col in df.columns:\n",
    "        # 1) 문자열/object -> datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=False)\n",
    "        # 2) tz-aware면 KST로 변환 후 tz 제거(naive)\n",
    "        try:\n",
    "            if pd.api.types.is_datetime64tz_dtype(df[col].dtype):\n",
    "                df[col] = df[col].dt.tz_convert(\"Asia/Seoul\").dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            # dtype 체크 실패 시 .dt.tz 존재 여부로 방어\n",
    "            if getattr(df[col].dt, \"tz\", None) is not None:\n",
    "                df[col] = df[col].dt.tz_convert(\"Asia/Seoul\").dt.tz_localize(None)\n",
    "        # 3) 여전히 tz-naive가 아니면 최후 수단으로 문자열 통일\n",
    "        if pd.api.types.is_object_dtype(df[col].dtype):\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# 1) test 쪽 time을 tz-naive로\n",
    "if \"time\" in test.columns:\n",
    "    test = test.copy()\n",
    "    test[\"time\"] = ensure_kst_series(test[\"time\"])  # KST로 맞추고 tz 제거\n",
    "\n",
    "# 2) sub 쪽 time을 tz-naive로\n",
    "if \"time\" in sub.columns:\n",
    "    sub = sub.copy()\n",
    "    normalize_time_key_inplace(sub, \"time\")\n",
    "\n",
    "if merge_keys:\n",
    "    # 키 정합성 빠른 경로: 키가 완전히 동일하면 인덱스 대입\n",
    "    left_keys  = sub[merge_keys].copy()\n",
    "    right_keys = test[merge_keys].copy()\n",
    "    # time이 있으면 양쪽 모두 tz-naive인지 재확인 (안전)\n",
    "    if \"time\" in merge_keys:\n",
    "        left_keys[\"time\"]  = pd.to_datetime(left_keys[\"time\"],  errors=\"coerce\")\n",
    "        right_keys[\"time\"] = pd.to_datetime(right_keys[\"time\"], errors=\"coerce\")\n",
    "\n",
    "    if left_keys.equals(right_keys):\n",
    "        out = sub.copy()\n",
    "        out[\"nins\"] = pred_te\n",
    "    else:\n",
    "        # 타입 불일치 방지: object vs datetime 충돌 방어\n",
    "        # 시간 키가 있으면 둘 다 문자열 포맷으로 통일 후 병합\n",
    "        lk = left_keys.copy()\n",
    "        rk = right_keys.copy()\n",
    "        if \"time\" in merge_keys:\n",
    "            fmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "            lk[\"time\"] = pd.to_datetime(lk[\"time\"], errors=\"coerce\").dt.strftime(fmt)\n",
    "            rk[\"time\"] = pd.to_datetime(rk[\"time\"], errors=\"coerce\").dt.strftime(fmt)\n",
    "\n",
    "        ref = rk.copy()\n",
    "        ref[\"nins\"] = pred_te\n",
    "        # sub의 키도 동일 포맷으로 치환\n",
    "        sub_merge = sub.copy()\n",
    "        if \"time\" in merge_keys:\n",
    "            sub_merge[\"time\"] = lk[\"time\"]\n",
    "\n",
    "        out = safe_merge(sub_merge.drop(columns=[\"nins\"], errors=\"ignore\"), ref, on=merge_keys, how=\"left\")\n",
    "else:\n",
    "    # 병합 키가 없으면 순서가 동일하다고 가정하고 바로 대입\n",
    "    out = sub.copy()\n",
    "    out[\"nins\"] = pred_te\n",
    "\n",
    "out[\"nins\"] = out[\"nins\"].fillna(0).clip(lower=0).astype(\"float32\")\n",
    "out.to_csv(SAVE_SUB, index=False)\n",
    "print(\"Saved:\", SAVE_SUB, out.shape, \"| null nins:\", int(out[\"nins\"].isna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3bd052e-cf33-4c71-b414-a773b2b24280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>G0h</td>\n",
       "      <td>6.857763e+07</td>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>humidity</td>\n",
       "      <td>2.486824e+07</td>\n",
       "      <td>17431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elev_sin</td>\n",
       "      <td>1.033187e+07</td>\n",
       "      <td>7376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>precip_3h_sum</td>\n",
       "      <td>6.978170e+06</td>\n",
       "      <td>4777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AM</td>\n",
       "      <td>6.305032e+06</td>\n",
       "      <td>3577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>temp_a</td>\n",
       "      <td>5.428024e+06</td>\n",
       "      <td>18741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doy</td>\n",
       "      <td>4.964510e+06</td>\n",
       "      <td>12935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dy_cos1</td>\n",
       "      <td>4.858070e+06</td>\n",
       "      <td>9477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hr_cos1</td>\n",
       "      <td>4.166779e+06</td>\n",
       "      <td>1684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_press</td>\n",
       "      <td>4.069883e+06</td>\n",
       "      <td>15112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dy_sin1</td>\n",
       "      <td>4.069809e+06</td>\n",
       "      <td>10898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wind_spd_a</td>\n",
       "      <td>2.852750e+06</td>\n",
       "      <td>16769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hour</td>\n",
       "      <td>2.839630e+06</td>\n",
       "      <td>5927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wind_dir_a</td>\n",
       "      <td>2.506976e+06</td>\n",
       "      <td>10900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rain</td>\n",
       "      <td>2.468127e+06</td>\n",
       "      <td>4237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wind_gust_spd</td>\n",
       "      <td>2.234662e+06</td>\n",
       "      <td>14232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wind_dir_a_sin</td>\n",
       "      <td>1.791195e+06</td>\n",
       "      <td>10779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wind_spd_b</td>\n",
       "      <td>1.511437e+06</td>\n",
       "      <td>11795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wind_dir_a_cos</td>\n",
       "      <td>1.500261e+06</td>\n",
       "      <td>9481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wind_dir_b</td>\n",
       "      <td>8.211936e+05</td>\n",
       "      <td>7174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature          gain  split\n",
       "11             G0h  6.857763e+07   4912\n",
       "4         humidity  2.486824e+07  17431\n",
       "9         elev_sin  1.033187e+07   7376\n",
       "12   precip_3h_sum  6.978170e+06   4777\n",
       "10              AM  6.305032e+06   3577\n",
       "6           temp_a  5.428024e+06  18741\n",
       "14             doy  4.964510e+06  12935\n",
       "18         dy_cos1  4.858070e+06   9477\n",
       "16         hr_cos1  4.166779e+06   1684\n",
       "3     ground_press  4.069883e+06  15112\n",
       "17         dy_sin1  4.069809e+06  10898\n",
       "8       wind_spd_a  2.852750e+06  16769\n",
       "13            hour  2.839630e+06   5927\n",
       "7       wind_dir_a  2.506976e+06  10900\n",
       "5             rain  2.468127e+06   4237\n",
       "1    wind_gust_spd  2.234662e+06  14232\n",
       "19  wind_dir_a_sin  1.791195e+06  10779\n",
       "2       wind_spd_b  1.511437e+06  11795\n",
       "20  wind_dir_a_cos  1.500261e+06   9481\n",
       "0       wind_dir_b  8.211936e+05   7174"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 14: Inspect feature importances =====\n",
    "imp2 = pd.DataFrame({\n",
    "    \"feature\": final_feats,\n",
    "    \"gain\": model2.booster_.feature_importance(importance_type=\"gain\"),\n",
    "    \"split\": model2.booster_.feature_importance(importance_type=\"split\"),\n",
    "}).sort_values(\"gain\", ascending=False)\n",
    "imp2.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759359f1-4dc3-4d8e-9260-1c6d472a8232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa837083-abe8-4906-b9df-d3ed6afbd1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c53e5e-5d83-4145-ba13-a483a0835899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[셀 1] 유틸 함수(자급자족 버전 – 외부 파일 없음)\n",
    "# === Feature Audit Utilities (self-contained) ===\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_Xy(df: pd.DataFrame,\n",
    "               target: str = \"nins\",\n",
    "               id_cols: Optional[List[str]] = None,\n",
    "               feature_cols: Optional[List[str]] = None,\n",
    "               drop_if_missing: bool = True) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Assemble X and y from a DataFrame.\n",
    "    - If feature_cols is None: use numeric columns except id_cols + target.\n",
    "    - If drop_if_missing: drop rows with NA in y (and in X if needed).\n",
    "    \"\"\"\n",
    "    id_cols = id_cols or []\n",
    "    if feature_cols is None:\n",
    "        numeric = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        feature_cols = [c for c in numeric if c not in set(id_cols + [target])]\n",
    "\n",
    "    X = df.loc[:, feature_cols].copy()\n",
    "    y = df.loc[:, target].copy()\n",
    "\n",
    "    if drop_if_missing:\n",
    "        keep = y.notna()\n",
    "        if X.isna().any().any():\n",
    "            keep &= ~(X.isna().any(axis=1))\n",
    "        X = X.loc[keep].reset_index(drop=True)\n",
    "        y = y.loc[keep].reset_index(drop=True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def compute_vif(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute VIFs by regressing each standardized feature against the others.\"\"\"\n",
    "    cols = X.columns.tolist()\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Z = scaler.fit_transform(X.values.astype(float))\n",
    "    Z = np.nan_to_num(Z, nan=0.0)\n",
    "\n",
    "    vif_values = []\n",
    "    for j in range(Z.shape[1]):\n",
    "        yj = Z[:, j]\n",
    "        Xj = np.delete(Z, j, axis=1)\n",
    "        reg = LinearRegression().fit(Xj, yj)\n",
    "        r2 = reg.score(Xj, yj)\n",
    "        vif = np.inf if r2 >= 0.999999 else 1.0 / (1.0 - r2)\n",
    "        vif_values.append(vif)\n",
    "\n",
    "    out = pd.DataFrame({\"feature\": cols, \"VIF\": vif_values}).sort_values(\"VIF\", ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def high_corr_pairs(X: pd.DataFrame, thresh: float = 0.98, method: str = \"pearson\") -> pd.DataFrame:\n",
    "    \"\"\"Return pairs of features with |corr| >= thresh (upper triangle only).\"\"\"\n",
    "    corr = X.corr(method=method).abs()\n",
    "    hits = []\n",
    "    cols = corr.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            v = corr.iat[i, j]\n",
    "            if np.isfinite(v) and v >= thresh:\n",
    "                hits.append((cols[i], cols[j], float(v)))\n",
    "    return pd.DataFrame(hits, columns=[\"feat_a\",\"feat_b\",\"abs_corr\"]).sort_values(\"abs_corr\", ascending=False)\n",
    "\n",
    "def fit_importance(X: pd.DataFrame, y: pd.Series,\n",
    "                   random_state: int = 42, n_jobs: int = -1):\n",
    "    \"\"\"Fit RandomForest and compute impurity & permutation importance.\"\"\"\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=None,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=random_state,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"impurity_importance\": rf.feature_importances_\n",
    "    }).sort_values(\"impurity_importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    perm = permutation_importance(rf, X_valid, y_valid,\n",
    "                                  n_repeats=10, random_state=random_state, n_jobs=n_jobs)\n",
    "    perm_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"perm_importance_mean\": perm.importances_mean,\n",
    "        \"perm_importance_std\": perm.importances_std\n",
    "    }).sort_values(\"perm_importance_mean\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return rf, imp_df, perm_df\n",
    "\n",
    "def summarize_importance(imp_df: pd.DataFrame, perm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge impurity and permutation importance, add average rank.\"\"\"\n",
    "    a = imp_df.copy()\n",
    "    b = perm_df.copy()\n",
    "    a[\"rank_impurity\"] = a[\"impurity_importance\"].rank(ascending=False, method=\"min\")\n",
    "    b[\"rank_perm\"]     = b[\"perm_importance_mean\"].rank(ascending=False, method=\"min\")\n",
    "    out = a.merge(b, on=\"feature\", how=\"outer\")\n",
    "    out[\"rank_avg\"] = out[[\"rank_impurity\",\"rank_perm\"]].mean(axis=1)\n",
    "    return out.sort_values(\"rank_avg\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "def suggest_feature_drops(X: pd.DataFrame,\n",
    "                          vif_df: pd.DataFrame,\n",
    "                          corr_pairs_df: pd.DataFrame,\n",
    "                          imp_summary_df: pd.DataFrame,\n",
    "                          vif_thresh: float = 10.0,\n",
    "                          corr_thresh: float = 0.98,\n",
    "                          bottom_quantile: float = 0.15) -> Dict[str, List[str]]:\n",
    "    \"\"\"Heuristic drop suggestions using VIF, high-corr, and low-importance.\"\"\"\n",
    "    by_vif = vif_df.loc[vif_df[\"VIF\"] >= vif_thresh, \"feature\"].tolist()\n",
    "\n",
    "    imp = imp_summary_df.copy()\n",
    "    imp[\"rank_avg\"] = imp[\"rank_avg\"].fillna(1e9)\n",
    "    cutoff = imp[\"rank_avg\"].quantile(1.0 - bottom_quantile)\n",
    "    by_imp = imp.loc[imp[\"rank_avg\"] >= cutoff, \"feature\"].tolist()\n",
    "\n",
    "    ranks = imp.set_index(\"feature\")[\"rank_avg\"].to_dict()\n",
    "    drops_corr = set()\n",
    "    for _, row in corr_pairs_df.iterrows():\n",
    "        a, b = row[\"feat_a\"], row[\"feat_b\"]\n",
    "        ra = ranks.get(a, 1e9)\n",
    "        rb = ranks.get(b, 1e9)\n",
    "        if ra >= rb:\n",
    "            drops_corr.add(a)\n",
    "        else:\n",
    "            drops_corr.add(b)\n",
    "\n",
    "    union = sorted(set(by_vif) | set(by_imp) | drops_corr)\n",
    "    return {\n",
    "        \"by_vif\": sorted(set(by_vif)),\n",
    "        \"by_corr\": sorted(drops_corr),\n",
    "        \"by_importance\": sorted(set(by_imp)),\n",
    "        \"union\": union,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30805d50-ec2e-401b-8b22-7099c6b48922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CSV: C:\\Users\\alexs\\OneDrive\\Desktop\\train.csv\n",
      "[READY] df shape=(19236948, 33), columns(sample)=['time', 'pv_id', 'appr_temp', 'ceiling', 'cloud_b', 'dew_point', 'precip_1h', 'pressure', 'real_feel_temp', 'real_feel_temp_shade', 'rel_hum', 'temp_b']\n"
     ]
    }
   ],
   "source": [
    "# === Bootstrap: ensure a training DataFrame `df` is available ===\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _list_dfs_in_memory():\n",
    "    return {k:v for k,v in globals().items() if isinstance(v, pd.DataFrame)}\n",
    "\n",
    "def _pick_df_with_target(dfs: dict, target=\"nins\"):\n",
    "    for name, d in dfs.items():\n",
    "        if target in d.columns:\n",
    "            return name, d\n",
    "    return None, None\n",
    "\n",
    "def _find_csv_candidates():\n",
    "    pats = [\n",
    "        \"./*.csv\",\"./data/*.csv\",\"../data/*.csv\",\n",
    "        \"./input/*.csv\",\"./dataset*.csv\",\"./datasets/*.csv\",\n",
    "        \"./**/train*.csv\",\"../**/train*.csv\"\n",
    "    ]\n",
    "    seen = set()\n",
    "    files = []\n",
    "    for p in pats:\n",
    "        for f in glob.glob(p, recursive=True):\n",
    "            fp = str(Path(f).resolve())\n",
    "            if fp not in seen and fp.lower().endswith(\".csv\"):\n",
    "                seen.add(fp); files.append(fp)\n",
    "    return files\n",
    "\n",
    "def _read_head_for_score(fp, target=\"nins\"):\n",
    "    try:\n",
    "        head = pd.read_csv(fp, nrows=512)\n",
    "        has_target = target in head.columns\n",
    "        name_score = 1 if \"train\" in Path(fp).name.lower() else 0\n",
    "        return has_target, name_score, head.columns.tolist()\n",
    "    except Exception:\n",
    "        return False, -1, []\n",
    "\n",
    "def bootstrap_training_df(target=\"nins\", prefer_in_memory=True):\n",
    "    # 1) 메모리 DF 우선\n",
    "    if prefer_in_memory:\n",
    "        dfs = _list_dfs_in_memory()\n",
    "        name, d = _pick_df_with_target(dfs, target=target)\n",
    "        if d is not None:\n",
    "            print(f\"[INFO] Found DataFrame in memory: `{name}` shape={d.shape}\")\n",
    "            return d.copy()\n",
    "\n",
    "    # 2) CSV 탐색\n",
    "    cands = _find_csv_candidates()\n",
    "    if not cands:\n",
    "        raise ValueError(\"학습 DataFrame도 없고, CSV 파일도 못 찾았어요. 데이터 로드 셀을 먼저 실행하거나 TRAIN_PATH를 지정해 주세요.\")\n",
    "\n",
    "    scored = []\n",
    "    for fp in cands:\n",
    "        has_target, name_score, cols = _read_head_for_score(fp, target=target)\n",
    "        # 우선순위: (타깃 포함 여부, 파일명에 'train' 포함, 파일 크기)\n",
    "        scored.append((has_target, name_score, Path(fp).stat().st_size, fp))\n",
    "    scored.sort(reverse=True)\n",
    "    best = scored[0][-1]\n",
    "\n",
    "    print(f\"[INFO] Loading CSV: {best}\")\n",
    "    df_loaded = pd.read_csv(best)\n",
    "    if target not in df_loaded.columns:\n",
    "        raise ValueError(f\"로드된 파일에 `{target}` 컬럼이 없어요. 다른 파일을 지정해 주세요.\\n(로드된 컬럼 예시: {list(df_loaded.columns[:12])})\")\n",
    "    return df_loaded\n",
    "\n",
    "# ---- 실행부 ----\n",
    "# 1) 사용자가 직접 경로를 지정하고 싶다면 ↓ 이 줄만 활성화해서 경로 입력\n",
    "# TRAIN_PATH = r\"./data/train.csv\"  # <- 직접 설정 (필요 시)\n",
    "\n",
    "if 'df' not in globals():\n",
    "    if 'TRAIN_PATH' in globals():\n",
    "        print(f\"[INFO] Loading TRAIN_PATH: {TRAIN_PATH}\")\n",
    "        df = pd.read_csv(TRAIN_PATH)\n",
    "    else:\n",
    "        df = bootstrap_training_df(target=\"nins\", prefer_in_memory=True)\n",
    "\n",
    "print(f\"[READY] df shape={df.shape}, columns(sample)={list(df.columns[:12])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2be4f0-256c-4c70-a7ce-342921ab6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using training DataFrame: `df`  shape=(19236948, 33)\n",
      "[INFO] Auto-inferred feature_cols (27 cols)\n",
      "[INFO] X: (1602878, 27), y: (1602878,)\n"
     ]
    }
   ],
   "source": [
    "#[셀 2] 네 노트북 변수명에 맞춰 학습용 DF 자동 선택 + 피처 후보 자동 구성\n",
    "# === Auto-pick training DataFrame (robust) & assemble features ===\n",
    "import pandas as pd\n",
    "\n",
    "# 0) 우선순위 이름 후보\n",
    "_name_priority = [\"train\", \"df_tr\", \"df_train\", \"df_trn\", \"train_df\", \"df_tr_all\", \"tr\", \"df\"]\n",
    "\n",
    "# 1) 메모리에서 DataFrame 스캔\n",
    "_df_name = None\n",
    "df_map = {k: v for k, v in globals().items() if isinstance(v, pd.DataFrame)}\n",
    "\n",
    "# 1-1) 우선순위 이름 중 존재하는 것 먼저 시도\n",
    "for nm in _name_priority:\n",
    "    if nm in df_map and \"nins\" in df_map[nm].columns:\n",
    "        _df_name = nm\n",
    "        break\n",
    "\n",
    "# 1-2) 실패하면, 메모리에 있는 DF 중에서 'nins' 컬럼 포함하는 것들 중 가장 큰 걸 선택\n",
    "if _df_name is None:\n",
    "    cands = [(k, v) for k, v in df_map.items() if \"nins\" in v.columns]\n",
    "    if cands:\n",
    "        _df_name = sorted(cands, key=lambda kv: (len(kv[1]), len(kv[1].columns)), reverse=True)[0][0]\n",
    "\n",
    "# 1-3) 그래도 없으면, 어떤 DF들이 있는지 보여주고 에러\n",
    "if _df_name is None:\n",
    "    if not df_map:\n",
    "        raise ValueError(\"메모리에서 pandas.DataFrame을 찾지 못했어요. 데이터 로드 셀을 먼저 실행해주세요.\")\n",
    "    else:\n",
    "        print(\"[디버그] 현재 메모리 DataFrame 목록과 주요 컬럼 예시:\")\n",
    "        for k, v in list(df_map.items())[:8]:\n",
    "            print(f\" - {k}: shape={v.shape}, columns(head)={list(v.columns[:10])}\")\n",
    "        raise ValueError(\"학습용 DF를 찾지 못했어요. 위 목록 중 'nins'를 포함하는 DF를 먼저 만들어 주세요. 예: df = <그 DF>\")\n",
    "\n",
    "# 2) 학습 DF 확정\n",
    "df = df_map[_df_name].copy()\n",
    "print(f\"[INFO] Using training DataFrame: `{_df_name}`  shape={df.shape}\")\n",
    "\n",
    "# 3) id/key 후보 (존재하는 것만)\n",
    "id_candidates = [\"time\",\"pv_id\",\"pv_id_code\",\"coord1\",\"coord2\"]\n",
    "id_cols = [c for c in id_candidates if c in df.columns]\n",
    "TARGET = \"nins\"\n",
    "\n",
    "# 4) 기존 피처 리스트가 있으면 존중, 없으면 자동 추론\n",
    "feature_cols = None\n",
    "if \"final_feats\" in globals() and isinstance(final_feats, (list, tuple)) and len(final_feats) > 0:\n",
    "    feature_cols = [c for c in final_feats if c in df.columns]\n",
    "    print(f\"[INFO] Using existing `final_feats` ({len(feature_cols)} cols)\")\n",
    "elif \"feature_cols\" in globals() and isinstance(feature_cols, (list, tuple)) and len(feature_cols) > 0:\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    print(f\"[INFO] Using existing `feature_cols` ({len(feature_cols)} cols)\")\n",
    "else:\n",
    "    numeric = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    feature_cols = [c for c in numeric if c not in set(id_cols + [TARGET])]\n",
    "    print(f\"[INFO] Auto-inferred feature_cols ({len(feature_cols)} cols)\")\n",
    "\n",
    "# 5) X, y 구성\n",
    "X, y = prepare_Xy(df, target=TARGET, id_cols=id_cols, feature_cols=feature_cols, drop_if_missing=True)\n",
    "print(f\"[INFO] X: {X.shape}, y: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d668eac3-541f-495e-9830-8ba8ae849aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>temp_a</td>\n",
       "      <td>1259.810335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temp_min</td>\n",
       "      <td>686.301592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>temp_max</td>\n",
       "      <td>588.513520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>real_feel_temp</td>\n",
       "      <td>470.292924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>real_feel_temp_shade</td>\n",
       "      <td>467.990421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>temp_b</td>\n",
       "      <td>405.947800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wind_chill_temp</td>\n",
       "      <td>186.264153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dew_point</td>\n",
       "      <td>141.324294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>appr_temp</td>\n",
       "      <td>98.874664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rel_hum</td>\n",
       "      <td>27.259746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ground_press</td>\n",
       "      <td>16.749770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pressure</td>\n",
       "      <td>15.220349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>uv_idx</td>\n",
       "      <td>7.189412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>humidity</td>\n",
       "      <td>5.518394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wind_spd_b</td>\n",
       "      <td>3.780592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wind_gust_spd</td>\n",
       "      <td>3.186037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wind_spd_a</td>\n",
       "      <td>2.243630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cloud_b</td>\n",
       "      <td>2.170354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ceiling</td>\n",
       "      <td>1.894845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cloud_a</td>\n",
       "      <td>1.764835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>energy</td>\n",
       "      <td>1.673677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>precip_1h</td>\n",
       "      <td>1.479313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rain</td>\n",
       "      <td>1.457007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wind_dir_a</td>\n",
       "      <td>1.312057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>wind_dir_b</td>\n",
       "      <td>1.164949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature          VIF\n",
       "0                 temp_a  1259.810335\n",
       "1               temp_min   686.301592\n",
       "2               temp_max   588.513520\n",
       "3         real_feel_temp   470.292924\n",
       "4   real_feel_temp_shade   467.990421\n",
       "5                 temp_b   405.947800\n",
       "6        wind_chill_temp   186.264153\n",
       "7              dew_point   141.324294\n",
       "8              appr_temp    98.874664\n",
       "9                rel_hum    27.259746\n",
       "10          ground_press    16.749770\n",
       "11              pressure    15.220349\n",
       "12                uv_idx     7.189412\n",
       "13              humidity     5.518394\n",
       "14            wind_spd_b     3.780592\n",
       "15         wind_gust_spd     3.186037\n",
       "16            wind_spd_a     2.243630\n",
       "17               cloud_b     2.170354\n",
       "18               ceiling     1.894845\n",
       "19               cloud_a     1.764835\n",
       "20                energy     1.673677\n",
       "21             precip_1h     1.479313\n",
       "22                  rain     1.457007\n",
       "23            wind_dir_a     1.312057\n",
       "24            wind_dir_b     1.164949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_a</th>\n",
       "      <th>feat_b</th>\n",
       "      <th>abs_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>temp_a</td>\n",
       "      <td>temp_min</td>\n",
       "      <td>0.999254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>temp_a</td>\n",
       "      <td>temp_max</td>\n",
       "      <td>0.999115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>temp_max</td>\n",
       "      <td>temp_min</td>\n",
       "      <td>0.998342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>temp_b</td>\n",
       "      <td>wind_chill_temp</td>\n",
       "      <td>0.995462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>real_feel_temp</td>\n",
       "      <td>real_feel_temp_shade</td>\n",
       "      <td>0.992946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>appr_temp</td>\n",
       "      <td>temp_b</td>\n",
       "      <td>0.992045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appr_temp</td>\n",
       "      <td>wind_chill_temp</td>\n",
       "      <td>0.987609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>real_feel_temp_shade</td>\n",
       "      <td>wind_chill_temp</td>\n",
       "      <td>0.985795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appr_temp</td>\n",
       "      <td>real_feel_temp_shade</td>\n",
       "      <td>0.983124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>real_feel_temp</td>\n",
       "      <td>wind_chill_temp</td>\n",
       "      <td>0.983022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>temp_b</td>\n",
       "      <td>temp_a</td>\n",
       "      <td>0.980823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>temp_b</td>\n",
       "      <td>temp_min</td>\n",
       "      <td>0.980770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>temp_b</td>\n",
       "      <td>temp_max</td>\n",
       "      <td>0.980715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>real_feel_temp_shade</td>\n",
       "      <td>temp_b</td>\n",
       "      <td>0.980621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feat_a                feat_b  abs_corr\n",
       "12                temp_a              temp_min  0.999254\n",
       "11                temp_a              temp_max  0.999115\n",
       "13              temp_max              temp_min  0.998342\n",
       "7                 temp_b       wind_chill_temp  0.995462\n",
       "3         real_feel_temp  real_feel_temp_shade  0.992946\n",
       "1              appr_temp                temp_b  0.992045\n",
       "2              appr_temp       wind_chill_temp  0.987609\n",
       "6   real_feel_temp_shade       wind_chill_temp  0.985795\n",
       "0              appr_temp  real_feel_temp_shade  0.983124\n",
       "4         real_feel_temp       wind_chill_temp  0.983022\n",
       "8                 temp_b                temp_a  0.980823\n",
       "10                temp_b              temp_min  0.980770\n",
       "9                 temp_b              temp_max  0.980715\n",
       "5   real_feel_temp_shade                temp_b  0.980621"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m     \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:1732\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1731\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1733\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:1646\u001b[39m, in \u001b[36mParallel._abort\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1645\u001b[39m     ensure_ready = \u001b[38;5;28mself\u001b[39m._managed_backend\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m     \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28mself\u001b[39m._aborted = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\_parallel_backends.py:725\u001b[39m, in \u001b[36mLokyBackend.abort_everything\u001b[39m\u001b[34m(self, ensure_ready)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_workers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;28mself\u001b[39m._workers = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\executor.py:98\u001b[39m, in \u001b[36mMemmappingExecutor.terminate\u001b[39m\u001b[34m(self, kill_workers)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._submit_resize_lock:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temp_folder_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_clean_temporary_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\_memmapping_reducer.py:675\u001b[39m, in \u001b[36mTemporaryResourcesManager._clean_temporary_resources\u001b[39m\u001b[34m(self, context_id, force, allow_non_empty)\u001b[39m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m context_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._cached_temp_folders):\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_temporary_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_non_empty\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\_memmapping_reducer.py:701\u001b[39m, in \u001b[36mTemporaryResourcesManager._clean_temporary_resources\u001b[39m\u001b[34m(self, context_id, force, allow_non_empty)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[43mdelete_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m     \u001b[38;5;66;03m# Forget the folder once it has been deleted\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\disk.py:115\u001b[39m, in \u001b[36mdelete_folder\u001b[39m\u001b[34m(folder_path, onerror, allow_non_empty)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m allow_non_empty:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     util.debug(\u001b[33m\"\u001b[39m\u001b[33mSuccessfully deleted \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(folder_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\shutil.py:852\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[39m\n\u001b[32m    850\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m onerror(func, path, exc_info)\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m \u001b[43m_rmtree_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monexc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\shutil.py:701\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, dir_fd, onexc)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     51\u001b[39m rf.fit(X_tr, y_tr)\n\u001b[32m     53\u001b[39m imp_df = pd.DataFrame({\n\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: X_fast.columns,\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimpurity_importance\u001b[39m\u001b[33m\"\u001b[39m: rf.feature_importances_\n\u001b[32m     56\u001b[39m }).sort_values(\u001b[33m\"\u001b[39m\u001b[33mimpurity_importance\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m perm = \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_va_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_va_sub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m perm_df = pd.DataFrame({\n\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: X_fast.columns,\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mperm_importance_mean\u001b[39m\u001b[33m\"\u001b[39m: perm.importances_mean,\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mperm_importance_std\u001b[39m\u001b[33m\"\u001b[39m: perm.importances_std\n\u001b[32m     66\u001b[39m }).sort_values(\u001b[33m\"\u001b[39m\u001b[33mperm_importance_mean\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     68\u001b[39m summary = summarize_importance(imp_df, perm_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\inspection\\_permutation_importance.py:288\u001b[39m, in \u001b[36mpermutation_importance\u001b[39m\u001b[34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[39m\n\u001b[32m    285\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m    286\u001b[39m baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m scores = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_permutation_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    305\u001b[39m         name: _create_importances_bunch(\n\u001b[32m    306\u001b[39m             baseline_score[name],\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_score\n\u001b[32m    311\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:1741\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1739\u001b[39m     \u001b[38;5;28mself\u001b[39m._running = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1740\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detach_generator_exit:\n\u001b[32m-> \u001b[39m\u001b[32m1741\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_terminate_and_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_remaining_outputs) > \u001b[32m0\u001b[39m:\n\u001b[32m   1744\u001b[39m     batched_results = _remaining_outputs.popleft()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\parallel.py:1408\u001b[39m, in \u001b[36mParallel._terminate_and_reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1406\u001b[39m \u001b[38;5;28mself\u001b[39m._calling = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._managed_backend:\n\u001b[32m-> \u001b[39m\u001b[32m1408\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\_parallel_backends.py:716\u001b[39m, in \u001b[36mLokyBackend.terminate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    713\u001b[39m         \u001b[38;5;66;03m# Don't terminate the workers as we want to reuse them in later\u001b[39;00m\n\u001b[32m    714\u001b[39m         \u001b[38;5;66;03m# calls, but cleanup the temporary resources that the Parallel call\u001b[39;00m\n\u001b[32m    715\u001b[39m         \u001b[38;5;66;03m# created. This 'hack' requires a private, low-level operation.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_workers\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_temp_folder_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_clean_temporary_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m         \u001b[38;5;28mself\u001b[39m._workers = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset_batch_stats()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\_memmapping_reducer.py:701\u001b[39m, in \u001b[36mTemporaryResourcesManager._clean_temporary_resources\u001b[39m\u001b[34m(self, context_id, force, allow_non_empty)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# Clean up the folder if possible, either if it is empty or\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# if none of the files in it are in used and allow_non_empty.\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[43mdelete_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m     \u001b[38;5;66;03m# Forget the folder once it has been deleted\u001b[39;00m\n\u001b[32m    703\u001b[39m     \u001b[38;5;28mself\u001b[39m._cached_temp_folders.pop(context_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\joblib\\disk.py:131\u001b[39m, in \u001b[36mdelete_folder\u001b[39m\u001b[34m(folder_path, onerror, allow_non_empty)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m err_count > RM_SUBDIRS_N_RETRY:\n\u001b[32m    127\u001b[39m         \u001b[38;5;66;03m# the folder cannot be deleted right now. It maybe\u001b[39;00m\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# because some temporary files have not been deleted\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# yet.\u001b[39;00m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRM_SUBDIRS_RETRY_TIME\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#[셀 3] VIF / 높은 상관쌍 / 중요도 계산 + 드랍 후보 제안 → 최종 피처\n",
    "# === FAST AUDIT: 빠르게 끝내는 버전 ===\n",
    "# 아이디어:\n",
    "#  1) VIF/상관은 전체로 계산(필요시 컬럼 60개 내로 제한)\n",
    "#  2) 중요도는 RandomForest n_estimators ↓, validation 서브샘플, permutation n_repeats=3\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 0) (선택) 피처 수가 매우 많으면 상위 60개만 우선 점검\n",
    "MAX_FEATS_FOR_FAST = 60\n",
    "if X.shape[1] > MAX_FEATS_FOR_FAST:\n",
    "    print(f\"[FAST] Too many features ({X.shape[1]}). Temporarily trimming to first {MAX_FEATS_FOR_FAST}.\")\n",
    "    X_fast = X.iloc[:, :MAX_FEATS_FOR_FAST].copy()\n",
    "else:\n",
    "    X_fast = X\n",
    "\n",
    "# 1) VIF / 높은 상관쌍\n",
    "vif = compute_vif(X_fast)\n",
    "display(vif.sort_values(\"VIF\", ascending=False).head(25))\n",
    "\n",
    "corr_hits = high_corr_pairs(X_fast, thresh=0.98)\n",
    "display(corr_hits.head(50))\n",
    "\n",
    "# 2) 중요도 (빠른 설정)\n",
    "#   - RF 트리 수 300\n",
    "#   - train/valid = 80/20\n",
    "#   - valid에서 최대 30,000행만 사용\n",
    "#   - permutation n_repeats=3\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_fast, y, test_size=0.2, random_state=42)\n",
    "MAX_VALID = 30_000\n",
    "if len(X_va) > MAX_VALID:\n",
    "    idx = np.random.RandomState(42).choice(len(X_va), size=MAX_VALID, replace=False)\n",
    "    X_va_sub = X_va.iloc[idx]\n",
    "    y_va_sub = y_va.iloc[idx]\n",
    "else:\n",
    "    X_va_sub = X_va\n",
    "    y_va_sub = y_va\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_tr, y_tr)\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": X_fast.columns,\n",
    "    \"impurity_importance\": rf.feature_importances_\n",
    "}).sort_values(\"impurity_importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    rf, X_va_sub, y_va_sub,\n",
    "    n_repeats=3, random_state=42, n_jobs=-1\n",
    ")\n",
    "perm_df = pd.DataFrame({\n",
    "    \"feature\": X_fast.columns,\n",
    "    \"perm_importance_mean\": perm.importances_mean,\n",
    "    \"perm_importance_std\": perm.importances_std\n",
    "}).sort_values(\"perm_importance_mean\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "summary = summarize_importance(imp_df, perm_df)\n",
    "display(summary.head(30))\n",
    "\n",
    "# 3) 드랍 제안 (임계값은 유지)\n",
    "drops = suggest_feature_drops(\n",
    "    X_fast, vif, corr_hits, summary,\n",
    "    vif_thresh=10.0, corr_thresh=0.98, bottom_quantile=0.15\n",
    ")\n",
    "print(\"[DROP SUGGESTIONS]\")\n",
    "for k, v in drops.items():\n",
    "    print(f\" - {k}: {len(v)}  {v[:20]}{' ...' if len(v)>20 else ''}\")\n",
    "\n",
    "proposed_drop = set(drops[\"union\"])\n",
    "\n",
    "# 4) 최종 피처 구성:\n",
    "#   - 만약 X_fast가 전체 피처의 부분집합이라면,\n",
    "#     먼저 full X 기준 manual_drop 반영 -> 그 다음 X_fast 기반 drops 적용\n",
    "_base_feats = list(X.columns)  # 전체 피처 기준\n",
    "manual_drop = [\n",
    "    \"temp_min\", \"temp_max\", \"real_feel_temp\", \"real_feel_temp_shade\",\n",
    "    \"temp_b\", \"wind_chill_temp\", \"appr_temp\",\n",
    "    \"rel_hum\",\n",
    "    \"pressure\",\n",
    "    \"dew_point\",\n",
    "]\n",
    "final_feats_pruned = [c for c in _base_feats if c not in manual_drop]\n",
    "\n",
    "# X_fast 기반 drop도 반영 (교집합만 적용)\n",
    "final_feats_new = [c for c in final_feats_pruned if c not in proposed_drop]\n",
    "print(f\"\\n[FINAL FEATS] kept={len(final_feats_new)}  dropped={len(set(_base_feats)-set(final_feats_new))}\")\n",
    "final_feats = final_feats_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fa067-828a-49ea-91f2-691773bfb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[셀 4] (옵션) 검증/테스트 DF 점검\n",
    "# === Optional: validate splits (df_va / df_te) ===\n",
    "\n",
    "for name in [\"df_va\", \"df_te\"]:\n",
    "    if name in globals():\n",
    "        _d = globals()[name]\n",
    "        missing = [c for c in final_feats if c not in _d.columns]\n",
    "        extra = [c for c in _d.columns if c not in final_feats + [\"time\",\"pv_id\",\"pv_id_code\",\"coord1\",\"coord2\", \"nins\"]]\n",
    "        print(f\"\\n[{name}] shape={_d.shape}\")\n",
    "        print(f\" - missing features in this split: {missing}\")\n",
    "        print(f\" - extra columns (not used as features): {extra[:20]}{' ...' if len(extra)>20 else ''}\")\n",
    "\n",
    "        # 검증셋에 타깃(nins)이 있으면 간단 MAE 체크(선형적이지 않아도 대략적 sanity check)\n",
    "        if \"nins\" in _d.columns:\n",
    "            Xv = _d.loc[:, [c for c in final_feats if c in _d.columns]].copy()\n",
    "            yv = _d[\"nins\"].copy()\n",
    "            # NaN 제거\n",
    "            keep = (~Xv.isna().any(axis=1)) & yv.notna()\n",
    "            Xv = Xv.loc[keep]; yv = yv.loc[keep]\n",
    "            if len(Xv) > 0:\n",
    "                preds = model.predict(Xv)\n",
    "                from sklearn.metrics import mean_absolute_error\n",
    "                mae = mean_absolute_error(yv, preds)\n",
    "                print(f\" - simple RF MAE on {name}: {mae:.4f}\")\n",
    "            else:\n",
    "                print(\" - (skip) no valid rows after NA filtering\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
