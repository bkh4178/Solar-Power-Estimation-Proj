{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40cb968-1bc9-4e5a-b830-7176bccb7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FINAL_FEATS: 20\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Imports & Config (pv-split version) ===\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from pandas.core.dtypes.dtypes import DatetimeTZDtype\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pd.set_option(\"display.width\", 160)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "TIME_COL   = \"time\"\n",
    "PV_COL     = \"pv_id\"\n",
    "TARGET     = \"nins\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ‚úÖ Í≥µÏãù ÏµúÏ¢Ö ÌîºÏ≤ò 20Í∞ú (51.35.pyÏôÄ ÎèôÏùº)\n",
    "FINAL_FEATS = [\n",
    " 'humidity','wind_gust_spd','hour','doy','wind_spd_b',\n",
    " 'ceiling','uv_idx','appr_temp','uv_cloud_adj','dow',\n",
    " 'hour_sin','doy_sin','is_rain','rain','hour_cos',\n",
    " 'doy_cos','snow','coord1','coord2','haze'\n",
    "]\n",
    "\n",
    "MAX_GAP   = 12     # 5Î∂Ñ Í∞ÑÍ≤© Í∏∞Ï§Ä 12Ïπ∏ ‚âà 1ÏãúÍ∞Ñ\n",
    "DAY_HOURS = (6,18) # Ï£ºÍ∞Ñ ÌîÑÎ°ùÏãú(Î≥¥Í∞Ñ/ÌÅ¥Îû®ÌîÑÏóê ÏÇ¨Ïö©)\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"‚úÖ FINAL_FEATS:\", len(FINAL_FEATS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92416a9-d8c2-4aa9-96fd-d1090356c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Time utils + time features ===\n",
    "def ensure_naive_datetime(s: pd.Series, tz=\"Asia/Seoul\") -> pd.Series:\n",
    "    if not is_datetime64_any_dtype(s):\n",
    "        s = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    if isinstance(s.dtype, DatetimeTZDtype):\n",
    "        try:\n",
    "            s = s.dt.tz_convert(tz).dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            s = s.dt.tz_localize(None)\n",
    "    return s\n",
    "\n",
    "def cyclical_encode(series: pd.Series, period: int):\n",
    "    angle = 2*np.pi*(series.astype(\"float32\") % period)/period\n",
    "    return np.sin(angle).astype(\"float32\"), np.cos(angle).astype(\"float32\")\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, time_col=TIME_COL) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[time_col] = ensure_naive_datetime(out[time_col])\n",
    "    out[\"hour\"] = out[time_col].dt.hour.astype(\"int16\")\n",
    "    out[\"dow\"]  = out[time_col].dt.dayofweek.astype(\"int8\")\n",
    "    out[\"doy\"]  = out[time_col].dt.dayofyear.astype(\"int16\")\n",
    "    out[\"hour_sin\"], out[\"hour_cos\"] = cyclical_encode(out[\"hour\"], 24)\n",
    "    out[\"doy_sin\"],  out[\"doy_cos\"]  = cyclical_encode(out[\"doy\"], 365)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed1ab08-cbfe-4f5d-91f5-f00fd6d14584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Feature wrapper (ensure 20 feats exist) ===\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = add_time_features(df)\n",
    "\n",
    "    # is_rain\n",
    "    if \"is_rain\" not in out.columns:\n",
    "        src = \"rain\" if \"rain\" in out.columns else None\n",
    "        out[\"is_rain\"] = (pd.to_numeric(out[src], errors=\"coerce\").fillna(0) > 0).astype(\"int8\") if src else np.int8(0)\n",
    "\n",
    "    # haze (Í∞ÄÏãúÍ±∞Î¶¨ Ïó≠Ïàò ÌòïÌÉúÍ∞Ä ÏóÜÏúºÎ©¥ 0)\n",
    "    if \"haze\" not in out.columns:\n",
    "        out[\"haze\"] = np.float32(0.0)\n",
    "\n",
    "    # uv_cloud_adj (ÌÅ¥ÎùºÏö∞Îìú ÏóÜÏúºÎ©¥ uv_idxÎ°ú ÎåÄÏ≤¥)\n",
    "    if \"uv_cloud_adj\" not in out.columns:\n",
    "        out[\"uv_cloud_adj\"] = pd.to_numeric(out.get(\"uv_idx\", 0.0), errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    # pv_id Ïπ¥ÌÖåÍ≥†Î¶¨(Î©îÎ™®Î¶¨‚Üì)\n",
    "    if PV_COL in out.columns and not pd.api.types.is_categorical_dtype(out[PV_COL]):\n",
    "        out[PV_COL] = out[PV_COL].astype(\"category\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd939f04-a110-466a-a224-56e3e0c4fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Interpolation (physics-aware, memory-light) ===\n",
    "def interpolate_weather(df: pd.DataFrame, max_gap: int=MAX_GAP) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    day_mask = out[\"hour\"].between(DAY_HOURS[0], DAY_HOURS[1])\n",
    "\n",
    "    # Ïù¥Î≤§Ìä∏Ìòï: is_rain (Î≥¥Í∞Ñ X, ÏßßÏùÄ Í≤∞ÏÜêÎßå Î≥¥ÏôÑ)\n",
    "    if \"is_rain\" in out.columns:\n",
    "        s = pd.to_numeric(out[\"is_rain\"], errors=\"coerce\")\n",
    "        s = out.groupby(PV_COL, observed=True)[\"is_rain\"].transform(lambda x: x.ffill().bfill()).astype(\"float32\")\n",
    "        for lev in ([PV_COL,\"hour\"], [PV_COL]):\n",
    "            if s.isna().any():\n",
    "                s = s.fillna(out.groupby(lev, observed=True)[\"is_rain\"].transform(\"median\"))\n",
    "        out[\"is_rain\"] = s.fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Ïó∞ÏÜçÌòï: ÏÑ†ÌòïÎ≥¥Í∞Ñ(uvÎäî Ï£ºÍ∞ÑÎßå), ÏûîÏó¨Îäî (pv,hour)->pv->Ï†ÑÏ≤¥ Ï§ëÏïôÍ∞í\n",
    "    cont_cols = [c for c in [\"humidity\",\"wind_spd_b\",\"wind_gust_spd\",\"ceiling\",\"appr_temp\",\"uv_idx\",\"haze\"] if c in out.columns]\n",
    "    for c in cont_cols:\n",
    "        s = pd.to_numeric(out[c], errors=\"coerce\").astype(\"float32\")\n",
    "        if c == \"uv_idx\":\n",
    "            s = s.where(day_mask, 0.0)\n",
    "        s_lin = out.groupby(PV_COL, observed=True)[c].transform(\n",
    "            lambda x: x.interpolate(method=\"linear\", limit=max_gap, limit_direction=\"both\")\n",
    "        ).astype(\"float32\")\n",
    "        if s_lin.isna().any():\n",
    "            s_lin = s_lin.fillna(out.groupby([PV_COL,\"hour\"], observed=True)[c].transform(\"median\"))\n",
    "        if s_lin.isna().any():\n",
    "            s_lin = s_lin.fillna(out.groupby(PV_COL, observed=True)[c].transform(\"median\"))\n",
    "        out[c] = s_lin.fillna(s_lin.median()).astype(\"float32\")\n",
    "\n",
    "    # rain/snow: Î≥¥Í∞Ñ ÏóÜÏù¥ 0 Ï±ÑÏõÄ + Î¨ºÎ¶¨Ï†Å ÌÅ¥Î¶Ω\n",
    "    for c in [\"rain\",\"snow\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).clip(lower=0).astype(\"float32\")\n",
    "\n",
    "    # Î¨ºÎ¶¨ Î≤îÏúÑ ÌÅ¥Î¶Ω\n",
    "    clip_map = {\"humidity\":(0,100), \"uv_idx\":(0,None), \"wind_spd_b\":(0,None), \"wind_gust_spd\":(0,None),\n",
    "                \"ceiling\":(0,None), \"appr_temp\":(None,None), \"haze\":(0,None)}\n",
    "    for c,(lo,hi) in clip_map.items():\n",
    "        if c in out.columns:\n",
    "            x = out[c].astype(\"float32\")\n",
    "            if lo is not None: x = np.maximum(x, lo)\n",
    "            if hi is not None: x = np.minimum(x, hi)\n",
    "            out[c] = x.astype(\"float32\")\n",
    "\n",
    "    # uv_cloud_adj Í∞±Ïã†(ÌÅ¥ÎùºÏö∞Îìú ÏóÜÏúºÎ©¥ Í∑∏ÎåÄÎ°ú)\n",
    "    if \"uv_idx\" in out.columns:\n",
    "        out[\"uv_cloud_adj\"] = pd.to_numeric(out.get(\"uv_cloud_adj\", out[\"uv_idx\"]), errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bde6482-5cab-4baa-af6f-dd70f1aabcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train raw shape: (19236948, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_9136\\3110150134.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if PV_COL in out.columns and not pd.api.types.is_categorical_dtype(out[PV_COL]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped NaN target rows: 0\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Load CSVs + basic FE ===\n",
    "train_raw = pd.read_csv(\"train.csv\", low_memory=True, memory_map=True)\n",
    "print(\"Train raw shape:\", train_raw.shape)\n",
    "\n",
    "train_raw = add_features(train_raw)\n",
    "\n",
    "# ÌÉÄÍπÉ Ï†ïÏ†ú\n",
    "train_raw[TARGET] = pd.to_numeric(train_raw[TARGET], errors=\"coerce\").astype(\"float32\")\n",
    "before = len(train_raw)\n",
    "train_raw = train_raw.dropna(subset=[TARGET])\n",
    "print(f\"Dropped NaN target rows: {before - len(train_raw)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1a704e-0b8d-4545-adde-33feaf8fc0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_9136\\329455208.py:8: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  rng.shuffle(pv_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PV SPLIT] #pv train=147  valid=36\n",
      "           rows train=15452628  valid=3784320\n",
      "Split ‚Üí (15452628, 20) (3784320, 20)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: PV-based split ‚Üí Interp per set (no leakage) ===\n",
    "\n",
    "def split_by_pv(df: pd.DataFrame, valid_frac: float = 0.2, random_state: int = RANDOM_SEED):\n",
    "    \"\"\"pv_id Îã®ÏúÑÎ°ú 8:2 Î∂ÑÌï†\"\"\"\n",
    "    df = df.copy()\n",
    "    pv_ids = df[PV_COL].dropna().unique()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(pv_ids)\n",
    "\n",
    "    n_va = max(1, int(len(pv_ids) * valid_frac))\n",
    "    va_pv = set(pv_ids[:n_va])\n",
    "    tr_pv = set(pv_ids[n_va:])\n",
    "\n",
    "    train_df = df[df[PV_COL].isin(tr_pv)].copy()\n",
    "    valid_df = df[df[PV_COL].isin(va_pv)].copy()\n",
    "\n",
    "    print(f\"[PV SPLIT] #pv train={len(tr_pv)}  valid={len(va_pv)}\")\n",
    "    print(f\"           rows train={len(train_df)}  valid={len(valid_df)}\")\n",
    "    return train_df, valid_df\n",
    "\n",
    "# üîÅ pv split Ïã§Ìñâ\n",
    "train_df, valid_df = split_by_pv(train_raw, valid_frac=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# ÏÑ∏Ìä∏Î≥Ñ Î≥¥Í∞Ñ\n",
    "train_df = interpolate_weather(train_df, max_gap=MAX_GAP)\n",
    "valid_df = interpolate_weather(valid_df, max_gap=MAX_GAP)\n",
    "\n",
    "# ÌïÑÏöîÌïú Ïó¥Îßå ÏñáÍ≤å + NaN 0 ÎåÄÏ≤¥\n",
    "use_cols_tr = [c for c in (FINAL_FEATS + [TARGET]) if c in train_df.columns]\n",
    "use_cols_va = [c for c in (FINAL_FEATS + [TARGET]) if c in valid_df.columns]\n",
    "\n",
    "train_df = train_df[use_cols_tr].copy()\n",
    "valid_df = valid_df[use_cols_va].copy()\n",
    "\n",
    "train_df[FINAL_FEATS] = train_df[FINAL_FEATS].astype(\"float32\").fillna(0)\n",
    "valid_df[FINAL_FEATS] = valid_df[FINAL_FEATS].astype(\"float32\").fillna(0)\n",
    "\n",
    "X_tr, y_tr = train_df[FINAL_FEATS], train_df[TARGET].values\n",
    "X_va, y_va = valid_df[FINAL_FEATS], valid_df[TARGET].values\n",
    "\n",
    "print(\"Split ‚Üí\", X_tr.shape, X_va.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44924db-230d-4a1a-89e3-2e497480ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training (pv-split)...\n",
      "[0]\ttrain-mae:232.73010\tvalid-mae:235.20144\n",
      "[200]\ttrain-mae:45.24286\tvalid-mae:50.04910\n",
      "[400]\ttrain-mae:41.82376\tvalid-mae:48.05577\n",
      "[600]\ttrain-mae:39.72670\tvalid-mae:46.60492\n",
      "[800]\ttrain-mae:38.32322\tvalid-mae:45.66859\n",
      "[1000]\ttrain-mae:37.13002\tvalid-mae:44.84501\n",
      "[1200]\ttrain-mae:36.17707\tvalid-mae:44.15453\n",
      "[1400]\ttrain-mae:35.38490\tvalid-mae:43.62530\n",
      "[1600]\ttrain-mae:34.71578\tvalid-mae:43.15866\n",
      "[1800]\ttrain-mae:34.12281\tvalid-mae:42.74580\n",
      "[2000]\ttrain-mae:33.53366\tvalid-mae:42.32984\n",
      "[2200]\ttrain-mae:33.06114\tvalid-mae:41.99356\n",
      "[2400]\ttrain-mae:32.60883\tvalid-mae:41.68624\n",
      "[2600]\ttrain-mae:32.18076\tvalid-mae:41.38463\n",
      "[2800]\ttrain-mae:31.76874\tvalid-mae:41.10036\n",
      "[2999]\ttrain-mae:31.42271\tvalid-mae:40.86899\n",
      "‚úÖ Best iter=3000  |  VALID MAE(pv)=40.8690\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Train with early stopping + VALID MAE (pv-split) ===\n",
    "dtr = xgb.DMatrix(X_tr, label=y_tr, feature_names=FINAL_FEATS)\n",
    "dva = xgb.DMatrix(X_va, label=y_va, feature_names=FINAL_FEATS)\n",
    "\n",
    "# 51.35.pyÏóêÏÑú Ïì∞Îçò ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"learning_rate\": 0.06,\n",
    "    \"max_depth\": 9,\n",
    "    \"min_child_weight\": 4,\n",
    "    \"subsample\": 0.75,\n",
    "    \"colsample_bytree\": 0.85,\n",
    "    \"reg_alpha\": 0.3,\n",
    "    \"reg_lambda\": 0.6,\n",
    "    \"random_state\": RANDOM_SEED,\n",
    "    # ÌïÑÏöîÏãú \"tree_method\": \"gpu_hist\"\n",
    "}\n",
    "\n",
    "print(\"üöÄ Training (pv-split)...\")\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtr,\n",
    "    num_boost_round=3000,\n",
    "    evals=[(dtr,\"train\"), (dva,\"valid\")],\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=200\n",
    ")\n",
    "\n",
    "best_n = bst.best_iteration + 1\n",
    "pred_va = bst.predict(dva, iteration_range=(0, bst.best_iteration+1)).astype(\"float32\")\n",
    "val_mae = mean_absolute_error(y_va, pred_va)\n",
    "print(f\"‚úÖ Best iter={best_n}  |  VALID MAE(pv)={val_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b875dfe-5aca-4048-bb0b-772f134f81eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054de785-2f42-4f74-9100-f12d157a84c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d79e1e-24c0-4fe1-a738-20a0f104f8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7711d6-fff8-46af-9754-695fc639091f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6e811-dbf8-4846-81d1-e3360325b40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e900ac27-2c18-414d-b7cb-ca9f81fa2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sample_by_pv(df: pd.DataFrame, frac_pv: float = 0.35, random_state: int = 0):\n",
    "    df = df.copy()\n",
    "    pv_ids = df[PV_COL].dropna().unique()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(pv_ids)\n",
    "\n",
    "    n_keep = max(1, int(len(pv_ids) * frac_pv))\n",
    "    keep_pv = set(pv_ids[:n_keep])\n",
    "\n",
    "    out = df[df[PV_COL].isin(keep_pv)].copy()\n",
    "    print(f\"[sample_by_pv] frac_pv={frac_pv} ‚Üí #pv={n_keep}, rows={len(out)}\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5954f84-a92e-46ef-9869-66f92408d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "def run_pv_split_once(seed: int, base_df: pd.DataFrame):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"‚ñ∂ PV-SPLIT EXPERIMENT  seed={seed}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    train_df, valid_df = split_by_pv(base_df, valid_frac=0.2, random_state=seed)\n",
    "\n",
    "    train_df = interpolate_weather(train_df, max_gap=MAX_GAP)\n",
    "    valid_df = interpolate_weather(valid_df, max_gap=MAX_GAP)\n",
    "\n",
    "    use_cols_tr = [c for c in (FINAL_FEATS + [TARGET]) if c in train_df.columns]\n",
    "    use_cols_va = [c for c in (FINAL_FEATS + [TARGET]) if c in valid_df.columns]\n",
    "\n",
    "    train_df = train_df[use_cols_tr].copy()\n",
    "    valid_df = valid_df[use_cols_va].copy()\n",
    "\n",
    "    train_df[FINAL_FEATS] = train_df[FINAL_FEATS].astype(\"float32\").fillna(0)\n",
    "    valid_df[FINAL_FEATS] = valid_df[FINAL_FEATS].astype(\"float32\").fillna(0)\n",
    "\n",
    "    X_tr, y_tr = train_df[FINAL_FEATS], train_df[TARGET].values.astype(\"float32\")\n",
    "    X_va, y_va = valid_df[FINAL_FEATS], valid_df[TARGET].values.astype(\"float32\")\n",
    "\n",
    "    print(f\"  - X_tr: {X_tr.shape}, X_va: {X_va.shape}\")\n",
    "\n",
    "    dtr = xgb.DMatrix(X_tr, label=y_tr, feature_names=FINAL_FEATS)\n",
    "    dva = xgb.DMatrix(X_va, label=y_va, feature_names=FINAL_FEATS)\n",
    "\n",
    "    if \"params\" in globals():\n",
    "        p = params.copy()\n",
    "    else:\n",
    "        p = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"mae\",\n",
    "            \"learning_rate\": 0.06,\n",
    "            \"max_depth\": 9,\n",
    "            \"min_child_weight\": 4,\n",
    "            \"subsample\": 0.75,\n",
    "            \"colsample_bytree\": 0.85,\n",
    "            \"reg_alpha\": 0.3,\n",
    "            \"reg_lambda\": 0.6,\n",
    "            \"random_state\": seed,\n",
    "            \"tree_method\": \"hist\",\n",
    "        }\n",
    "\n",
    "    print(\"  üöÄ Training XGBoost (sampled pv set)...\")\n",
    "    bst = xgb.train(\n",
    "        p,\n",
    "        dtr,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtr, \"train\"), (dva, \"valid\")],\n",
    "        early_stopping_rounds=150,\n",
    "        verbose_eval=300,\n",
    "    )\n",
    "\n",
    "    best_n = bst.best_iteration + 1\n",
    "\n",
    "    pred_tr = bst.predict(dtr, iteration_range=(0, bst.best_iteration+1)).astype(\"float32\")\n",
    "    pred_va = bst.predict(dva, iteration_range=(0, bst.best_iteration+1)).astype(\"float32\")\n",
    "\n",
    "    mae_tr = mean_absolute_error(y_tr, pred_tr)\n",
    "    mae_va = mean_absolute_error(y_va, pred_va)\n",
    "\n",
    "    print(f\"  ‚úÖ seed={seed} | best_iter={best_n}\")\n",
    "    print(f\"     train MAE = {mae_tr:.4f}\")\n",
    "    print(f\"     valid MAE = {mae_va:.4f}\")\n",
    "    print(f\"     gap       = {mae_va - mae_tr:.4f}\")\n",
    "\n",
    "    del train_df, valid_df, X_tr, X_va, y_tr, y_va, dtr, dva, bst, pred_tr, pred_va\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"best_iter\": best_n,\n",
    "        \"train_mae\": mae_tr,\n",
    "        \"valid_mae\": mae_va,\n",
    "        \"gap\": mae_va - mae_tr,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18668e5c-c9a7-40c9-81f4-e4ca8ae4e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_9136\\3298655356.py:9: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  rng.shuffle(pv_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sample_by_pv] frac_pv=0.35 ‚Üí #pv=64, rows=6727680\n",
      "\n",
      "==============================\n",
      "‚ñ∂ PV-SPLIT EXPERIMENT  seed=42\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_9136\\329455208.py:8: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  rng.shuffle(pv_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PV SPLIT] #pv train=52  valid=12\n",
      "           rows train=5466240  valid=1261440\n",
      "  - X_tr: (5466240, 20), X_va: (1261440, 20)\n",
      "  üöÄ Training XGBoost (sampled pv set)...\n",
      "[0]\ttrain-mae:233.12773\tvalid-mae:230.93798\n",
      "[300]\ttrain-mae:39.36338\tvalid-mae:52.47251\n",
      "[600]\ttrain-mae:35.62052\tvalid-mae:50.55576\n",
      "[900]\ttrain-mae:33.34644\tvalid-mae:49.38522\n",
      "[1200]\ttrain-mae:31.67432\tvalid-mae:48.54135\n",
      "[1500]\ttrain-mae:30.39731\tvalid-mae:47.89674\n",
      "[1800]\ttrain-mae:29.34301\tvalid-mae:47.37417\n",
      "[1999]\ttrain-mae:28.69869\tvalid-mae:47.06550\n",
      "  ‚úÖ seed=42 | best_iter=2000\n",
      "     train MAE = 28.6987\n",
      "     valid MAE = 47.0655\n",
      "     gap       = 18.3668\n",
      "\n",
      "==============================\n",
      "‚ñ∂ PV-SPLIT EXPERIMENT  seed=7\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_9136\\329455208.py:8: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  rng.shuffle(pv_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PV SPLIT] #pv train=52  valid=12\n",
      "           rows train=5466240  valid=1261440\n",
      "  - X_tr: (5466240, 20), X_va: (1261440, 20)\n",
      "  üöÄ Training XGBoost (sampled pv set)...\n",
      "[0]\ttrain-mae:232.34753\tvalid-mae:233.64794\n",
      "[300]\ttrain-mae:40.76926\tvalid-mae:46.42538\n",
      "[600]\ttrain-mae:36.89537\tvalid-mae:44.28922\n",
      "[900]\ttrain-mae:34.61241\tvalid-mae:43.10720\n",
      "[1200]\ttrain-mae:32.95466\tvalid-mae:42.26582\n",
      "[1500]\ttrain-mae:31.61984\tvalid-mae:41.64640\n",
      "[1800]\ttrain-mae:30.52525\tvalid-mae:41.15068\n",
      "[1999]\ttrain-mae:29.90407\tvalid-mae:40.89233\n",
      "  ‚úÖ seed=7 | best_iter=2000\n",
      "     train MAE = 29.9041\n",
      "     valid MAE = 40.8923\n",
      "     gap       = 10.9883\n",
      "\n",
      "===== PV-SPLIT SEED COMPARISON (on sampled PVs) =====\n",
      "   seed  best_iter  train_mae  valid_mae        gap\n",
      "0    42       2000  28.698689  47.065495  18.366806\n",
      "1     7       2000  29.904074  40.892326  10.988253\n"
     ]
    }
   ],
   "source": [
    "# 1) train_raw ÏùºÎ∂ÄÎßå ÏÉòÌîåÎßÅ (Ïòà: pvÏùò 35%)\n",
    "train_sample = sample_by_pv(train_raw, frac_pv=0.35, random_state=0)\n",
    "\n",
    "# 2) seed=42, 7 Í∞ÅÍ∞Å ÎèåÎ†§Î≥¥Í∏∞\n",
    "results = []\n",
    "for sd in [42, 7]:\n",
    "    res = run_pv_split_once(sd, base_df=train_sample)\n",
    "    results.append(res)\n",
    "\n",
    "# 3) Í≤∞Í≥º ÏöîÏïΩ\n",
    "import pandas as pd\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n===== PV-SPLIT SEED COMPARISON (on sampled PVs) =====\")\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206ee2c-358b-43de-80cc-51008fe11092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d6b5a-9039-4af7-9d94-79c9153d6aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6dbcdd-0f04-4045-8566-c767c8d1e91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b977059-44b6-4e8f-9454-e650ee238d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2bdbd-b441-492c-89c3-4bfb6cde0ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc4aeb-a3b4-4639-9ec5-86f61f940f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432757b1-4fce-4508-a3fd-cd9bac3f7643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Full-data training with num_boost_round=3150 ...\n",
      "‚úÖ FULL Train MAE = 31.9649  |  (ref pv-valid MAE=40.8690)\n",
      "üíæ Saved ‚Üí xgb_full_final_pv.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell F1: Full-data train (use best_n*1.05, clipped) ===\n",
    "# train_raw, FINAL_FEATS, interpolate_weather, params Í∞Ä Ïïû ÏÖÄÏóêÏÑú Ïù¥ÎØ∏ Ï†ïÏùòÎê®\n",
    "\n",
    "# 1) full-data Î≥¥Í∞Ñ\n",
    "train_full = interpolate_weather(train_raw, max_gap=MAX_GAP)\n",
    "use_cols = [c for c in (FINAL_FEATS + [TARGET]) if c in train_full.columns]\n",
    "train_full = train_full[use_cols].copy()\n",
    "train_full[FINAL_FEATS] = train_full[FINAL_FEATS].astype(\"float32\").fillna(0)\n",
    "\n",
    "dfull = xgb.DMatrix(train_full[FINAL_FEATS], label=train_full[TARGET].values, feature_names=FINAL_FEATS)\n",
    "\n",
    "# 2) ÎùºÏö¥Îìú Ïàò: best_n * 1.05, [1800, 4200]Î°ú ÌÅ¥Î¶¨Ìïë\n",
    "num_round = int(best_n * 1.05)\n",
    "num_round = max(1800, min(4200, num_round))\n",
    "\n",
    "print(f\"üöÄ Full-data training with num_boost_round={num_round} ...\")\n",
    "final_bst = xgb.train(params, dfull, num_boost_round=num_round, verbose_eval=300)\n",
    "\n",
    "# train MAE Ìïú Î≤à Ï∞çÏñ¥Î≥¥Í∏∞ (Í≥ºÏ†ÅÌï© Ï†ïÎèÑ Í∞ê)\n",
    "pred_full = final_bst.predict(dfull).astype(\"float32\")\n",
    "mae_full = mean_absolute_error(train_full[TARGET].values, pred_full)\n",
    "print(f\"‚úÖ FULL Train MAE = {mae_full:.4f}  |  (ref pv-valid MAE={val_mae:.4f})\")\n",
    "\n",
    "final_bst.save_model(\"xgb_full_final_pv.json\")\n",
    "print(\"üíæ Saved ‚Üí xgb_full_final_pv.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66165dc5-e10b-493c-ac04-ee3225038f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_9136\\3110150134.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if PV_COL in out.columns and not pd.api.types.is_categorical_dtype(out[PV_COL]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: result_submission_pvsplit.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell F2: Predict test + save submission (pv model) ===\n",
    "TEST_CSV = \"test.csv\"\n",
    "SUB_CSV  = \"submission_sample.csv\" if os.path.exists(\"submission_sample.csv\") else \"sample_submission.csv\"\n",
    "\n",
    "test_raw = pd.read_csv(TEST_CSV, low_memory=True, memory_map=True)\n",
    "test_raw = add_features(test_raw)\n",
    "test_itp = interpolate_weather(test_raw, max_gap=MAX_GAP)\n",
    "\n",
    "# ÏûÖÎ†• Ï†ïÎ¶¨\n",
    "feat_exist = [c for c in FINAL_FEATS if c in test_itp.columns]\n",
    "test_mat = test_itp[feat_exist].astype(\"float32\").fillna(0)\n",
    "dte = xgb.DMatrix(test_mat, feature_names=feat_exist)\n",
    "\n",
    "# ÏòàÏ∏° + ÌÅ¥Î¶¨Ìïë\n",
    "pred = final_bst.predict(dte).astype(\"float32\")\n",
    "pred = np.clip(pred, 0, None)\n",
    "\n",
    "# (ÏÑ†ÌÉù) ÏïºÍ∞Ñ Î≥¥ÏàòÏ†Å ÌÅ¥Îû®ÌîÑ: uv_idx<=0 ÎòêÎäî ÏãúÍ∞Ñ not in [6,18] ‚Üí 0\n",
    "if \"uv_idx\" in test_itp.columns and \"hour\" in test_raw.columns:\n",
    "    mask_night = (pd.to_numeric(test_itp[\"uv_idx\"], errors=\"coerce\") <= 0) | ~test_raw[\"hour\"].between(6,18)\n",
    "    pred[mask_night.values] = 0.0\n",
    "\n",
    "# Ï†úÏ∂ú Ìè¨Îß∑ ÎßûÏ∂îÍ∏∞\n",
    "sub = pd.read_csv(SUB_CSV)\n",
    "if len(sub) == len(pred):\n",
    "    sub[\"nins\"] = pred\n",
    "else:\n",
    "    merge_keys = [k for k in [\"time\",\"pv_id\",\"type\"] if k in sub.columns and k in test_raw.columns]\n",
    "    for df_ in (sub, test_raw):\n",
    "        if \"time\" in df_.columns:\n",
    "            df_[\"time\"] = pd.to_datetime(df_[\"time\"], errors=\"coerce\")\n",
    "    ref = test_raw[merge_keys].copy()\n",
    "    ref[\"nins\"] = pred\n",
    "    sub = sub.drop(columns=[\"nins\"], errors=\"ignore\").merge(ref, on=merge_keys, how=\"left\")\n",
    "\n",
    "sub[\"nins\"] = sub[\"nins\"].fillna(0).astype(\"float32\")\n",
    "sub.to_csv(\"result_submission_pvsplit.csv\", index=False)\n",
    "print(\"‚úÖ Saved: result_submission_pvsplit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ead7cb-6689-464e-aa22-322bb7477c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
